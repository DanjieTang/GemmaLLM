{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c799cdf9-7e32-460b-9f0d-5b64b4839ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danjietang/Documents/Github/GemmaLLM/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a73cdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import math\n",
    "from transformers import AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2660dd2d-868b-44d9-8842-f652c749286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_context_length = 51\n",
    "device = \"mps\"\n",
    "epochs = 1\n",
    "batch_size = 256\n",
    "validation_batch_size = 256\n",
    "weight_decay = 1e-3\n",
    "lr = 1e-3\n",
    "num_layer = 3\n",
    "head_dim = 64\n",
    "projection_dim = 512\n",
    "expansion_factor = 16\n",
    "checkpoint_filepath = \"\"\n",
    "q_head = 8\n",
    "kv_head = 4\n",
    "training_data_path = \"languages_tokenized_50_train.npy\"\n",
    "eval_data_path = \"languages_tokenized_50_eval.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99869320-6624-4b85-9bf1-c982adc6a947",
   "metadata": {},
   "source": [
    "# Load llama2 token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0878240-ec08-4df2-a4ce-a03793b4f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097cdf0-fc3a-4089-a0ce-53a968012f1d",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b66a5b-1c5e-4b96-85e2-62cffd1224e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # Create memory-mapped array\n",
    "        self.mmap_data = np.load(filename, mmap_mode='r')\n",
    "        self.length = self.mmap_data.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.mmap_data[idx])\n",
    "\n",
    "training_dataset = Dataset(training_data_path)\n",
    "eval_dataset = Dataset(eval_data_path)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(eval_dataset, batch_size=validation_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6301f17-1ea0-4394-be58-14ad70344f13",
   "metadata": {},
   "source": [
    "# LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d780b67b-5d0b-479e-af1f-4ffdda4ab810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROPEEmbedding(nn.Module):\n",
    "    def __init__(self, max_context_length: int, head_dim: int = 64, theta: int = 10000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.create_embedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_embedding(max_context_length: int, head_dim: int = 64, theta: int = 10000) -> torch.Tensor:\n",
    "        # Angles\n",
    "        tensor = torch.arange(0, head_dim // 2)\n",
    "        tensor = torch.repeat_interleave(tensor, 2)\n",
    "        tensor = -tensor * 2 / head_dim\n",
    "        tensor = torch.pow(theta, tensor)\n",
    "\n",
    "        index = torch.arange(max_context_length).float() # This is the m in the formula\n",
    "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
    "\n",
    "        cos_matrix = tensor.cos()\n",
    "        sin_matrix = tensor.sin()\n",
    "        sin_matrix[0::2, :] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
    "\n",
    "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
    "        pos_emb = pos_emb.transpose(1, 0)\n",
    "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def flip_for_sin(tensor: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, S, D = tensor.shape\n",
    "        # original_shape = tensor.shape\n",
    "        tensor = tensor.view(B, H, S, D//2, 2) # Get to pairs\n",
    "        # tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
    "        tensor = tensor[:, :, :, :, [1, 0]].contiguous() # Swap\n",
    "        # tensor = tensor[..., [1, 0]] \n",
    "        tensor = tensor.view(B, H, S, D) # Get back to original shape\n",
    "        # tensor = tensor.reshape(original_shape) \n",
    "        return tensor\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
    "\n",
    "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
    "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
    "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
    "        tensor = cos + sin\n",
    "        return tensor\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.embedding = embedding\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "\n",
    "        # LoRA\n",
    "        self.lora_scale = lora_alpha / lora_rank\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "        # Gated attention from Qwen paper\n",
    "        # Each gate gets one gate score as recommended in the paper\n",
    "        self.gate = nn.Linear(hidden_dim, q_head)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "        pre_norm_tensor = tensor # Used to calculate gate score\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Apply ROPE\n",
    "        query = self.embedding(query)\n",
    "        key = self.embedding(key)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Gated attention from Qwen paper\n",
    "        gate_score = self.gate(pre_norm_tensor)\n",
    "        gate_score = F.sigmoid(gate_score)\n",
    "        gate_score = gate_score.repeat_interleave(self.head_dim, dim=-1)\n",
    "        value = value * gate_score\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(value)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "        # LoRA\n",
    "        self.lora_scale = lora_alpha / lora_rank\n",
    "        self.lora_gate_and_up_a = nn.Linear(hidden_size, lora_rank)\n",
    "        self.lora_gate_and_up_b = nn.Linear(lora_rank, hidden_size * expansion_factor * 2)\n",
    "        self.lora_down_a = nn.Linear(hidden_size * expansion_factor, lora_rank)\n",
    "        self.lora_down_b = nn.Linear(lora_rank, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        gate_and_up = self.gate_and_up(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_gate_and_up_a(tensor)\n",
    "            lora_tensor = self.lora_gate_and_up_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            gate_and_up = gate_and_up + lora_tensor\n",
    "        gate, up = gate_and_up.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        down_tensor = self.down(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_down_a(tensor)\n",
    "            lora_tensor = self.lora_down_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            down_tensor = down_tensor + lora_tensor\n",
    "        return down_tensor\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1, lora_rank: int = 16, lora_alpha: int = 32, device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, lora_alpha=lora_alpha) for _ in range(num_experts)])\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).to(self.device)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i], fine_tuning) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i], fine_tuning) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(self.device)\n",
    "        flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(self.device)\n",
    "        flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "class LLMLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.mqa = Attention(hidden_dim, head_dim, q_head, kv_head, embedding, lora_rank=lora_rank, lora_alpha=lora_alpha)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, num_experts=num_experts, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, device=device)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, lora_alpha=lora_alpha)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.mqa(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor, fine_tuning=fine_tuning)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor, fine_tuning=fine_tuning)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=self.device)# If not using MoE, load-balancing loss is zero\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 vocabulary_size: int,\n",
    "                 max_context_length: int,\n",
    "                 hidden_dim: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.embedding = ROPEEmbedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(LLMLayer(hidden_dim, head_dim, q_head, kv_head, self.embedding, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, use_moe=use_moe, num_experts=num_experts, lora_rank=lora_rank, lora_alpha=lora_alpha, device=device))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, vocabulary_size)\n",
    "        self.device = device\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"pos_emb\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, causal_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            tensor, load_balancing_loss = layer(tensor, attention_mask=causal_mask, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Classification\n",
    "        tensor = self.output_norm(tensor)\n",
    "        tensor = self.classifier(tensor)\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class VLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 max_context_length: int,\n",
    "                 word_embeddings_tensor: str,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 projection_dim: int = None,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Right now this code is hard coded to use CLIP ViT\n",
    "        model_id = \"openai/clip-vit-large-patch14\"\n",
    "        self.vision_model = CLIPVisionModel.from_pretrained(model_id)\n",
    "        self.vision_processor = CLIPImageProcessor.from_pretrained(model_id)\n",
    "        image_dim = 1024\n",
    "        self.visual_seq_len = 1# Vision sequence length\n",
    "\n",
    "        # Load model token embeddings\n",
    "        self.word_embeddings_tensor = torch.load(word_embeddings_tensor)\n",
    "        zero_row = torch.zeros(1, self.word_embeddings_tensor.shape[1])\n",
    "        self.word_embeddings_tensor = torch.cat((self.word_embeddings_tensor, zero_row), dim=0).to(device)\n",
    "        self.vocabulary_size, text_dim = self.word_embeddings_tensor.shape\n",
    "        self.word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "        # The token that seperates image tokens from text tokens\n",
    "        if projection_dim:\n",
    "            self.seperation_token = nn.Parameter(torch.randn(1, projection_dim))\n",
    "        else:\n",
    "            self.seperation_token = nn.Parameter(torch.randn(1, text_dim))\n",
    "\n",
    "        # Potentially image&text token projection layer\n",
    "        if projection_dim:\n",
    "            self.image_projection = image_dim != projection_dim\n",
    "            self.text_projection = text_dim != projection_dim\n",
    "        else: # If no explicit projection_dim, use text token_dim\n",
    "            self.image_projection = image_dim != text_dim\n",
    "            self.text_projection = False\n",
    "        if projection_dim:\n",
    "            if self.image_projection:\n",
    "                self.image_token_projection = nn.Linear(image_dim, projection_dim)\n",
    "            if self.text_projection:\n",
    "                self.text_token_projection = nn.Linear(text_dim, projection_dim)\n",
    "        else: \n",
    "            if self.image_projection:\n",
    "                self.image_token_projection = nn.Linear(image_dim, text_dim)\n",
    "\n",
    "        # Create the LLM\n",
    "        self.llm = LLM(num_layer, self.vocabulary_size, max_context_length, projection_dim if projection_dim else text_dim, expansion_factor=expansion_factor, head_dim=head_dim, q_head=q_head, kv_head=kv_head, dropout_ratio=dropout_ratio, theta=theta, use_moe=use_moe, num_experts=num_experts, load_balancing_loss_weight=load_balancing_loss_weight, fine_tuning=fine_tuning, lora_rank=lora_rank, lora_alpha=lora_alpha, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, image_paths: list[str | None]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, text_seq_len = token_ids.shape\n",
    "        pad_amount = self.visual_seq_len + 1\n",
    "        multimodal_seq_len = text_seq_len + pad_amount\n",
    "        assert batch_size == len(image_paths), \"Mismatch between text and image inputs\"\n",
    "\n",
    "        # 1. Preallocate input data\n",
    "        multimodal_input_tensor = torch.empty(\n",
    "            (batch_size, multimodal_seq_len, self.llm.classifier.in_features), \n",
    "            device=self.device, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # 2. Vectorized Text Embedding & Projection\n",
    "        # input_embeddings.shape = [batch, seq_len, emb_dim]\n",
    "        input_embeddings = self.word_embeddings_tensor[token_ids].float()\n",
    "        if self.text_projection:\n",
    "            input_embeddings = self.text_token_projection(input_embeddings)\n",
    "        \n",
    "        # Place text embeddings into the right side of the pre-allocated tensor\n",
    "        multimodal_input_tensor[:, pad_amount:, :] = input_embeddings\n",
    "\n",
    "        # 3. Collect images\n",
    "        has_image_mask = torch.tensor([p is not None for p in image_paths], device=self.device)\n",
    "        image_indices = torch.where(has_image_mask)[0]\n",
    "\n",
    "        # Prepare the padding embedding (for batches without images)\n",
    "        pad_token_emb = self.word_embeddings_tensor[-1:].float()\n",
    "        if self.text_projection:\n",
    "            pad_token_emb = self.text_token_projection(pad_token_emb)\n",
    "        \n",
    "        # Fill the left side with padding by default\n",
    "        multimodal_input_tensor[:, :pad_amount, :] = pad_token_emb.repeat(batch_size, pad_amount, 1)\n",
    "\n",
    "        if len(image_indices) > 0:\n",
    "            # Process only the actual images found\n",
    "            actual_images = [Image.open(image_paths[i]).convert(\"RGB\") for i in image_indices.tolist()]\n",
    "            vision_inputs = self.vision_processor(images=actual_images, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.vision_model(**vision_inputs)\n",
    "                visual_tokens = outputs.last_hidden_state[:, 0:self.visual_seq_len, :]\n",
    "            \n",
    "            if self.image_projection:\n",
    "                visual_tokens = self.image_token_projection(visual_tokens)\n",
    "                \n",
    "            # Scatter visual tokens and separation tokens into the multimodal tensor\n",
    "            multimodal_input_tensor[image_indices, :self.visual_seq_len, :] = visual_tokens\n",
    "            multimodal_input_tensor[image_indices, self.visual_seq_len, :] = self.seperation_token\n",
    "\n",
    "        # 4. 3D Causal Mask\n",
    "        causal_mask = torch.triu(torch.ones(multimodal_seq_len, multimodal_seq_len) * float('-inf'), diagonal=1).to(device)\n",
    "        causal_mask.requires_grad = False\n",
    "        \n",
    "        # 5. Forward to LLM\n",
    "        tensor, load_balancing_loss = self.llm(multimodal_input_tensor, causal_mask)\n",
    "        tensor = tensor[:, pad_amount:, :]\n",
    "        tensor = tensor.contiguous()\n",
    "    \n",
    "        return tensor, load_balancing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f4c49-f6a1-4b7c-a028-16440d116502",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf49fd2e-f6d8-41e3-91a4-af480f249b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_to_save = model.module\n",
    "    else:\n",
    "        model_to_save = model\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_to_save.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'checkpoint_{epoch}_{loss}.pth.tar'\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f'Checkpoint saved at epoch {epoch} as {filename}')\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename) -> int:\n",
    "    checkpoint = torch.load(filename, weights_only=False)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Checkpoint loaded from epoch {epoch} with loss {loss}')\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b561ab-1778-4c6c-9e3d-c928ef711fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VLM(num_layer, max_context_length, 'word_embeddings_tensor_llama3.pt', projection_dim=projection_dim, expansion_factor=expansion_factor, use_moe=False, q_head=q_head, kv_head=kv_head, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a08e515-f4bd-4e99-a408-c16b09ee0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 413195097 parameters.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "total_steps = epochs*len(training_loader)\n",
    "warmup_steps = int(total_steps * 0.01)\n",
    "\n",
    "# Warmup: LR linearly increases from 0 → base LR over warmup_steps\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_steps)\n",
    "\n",
    "# Cosine annealing: after warmup\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=(total_steps - warmup_steps), eta_min=3e-5)\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "\n",
    "if checkpoint_filepath != None and checkpoint_filepath != \"\":\n",
    "    current_epoch = load_checkpoint(llm, optimizer, checkpoint_filepath) + 1\n",
    "else:\n",
    "    current_epoch = 0\n",
    "\n",
    "print(\"This model has\", sum(p.numel() for p in vit.parameters()), \"parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf7e524-e5f1-4b9b-8955-9fb7488dfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212849a-fad8-4fc9-bd48-5348a45f93c9",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fdf79c-c835-4fc1-82dd-76927727f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 78125/78125 [19:18:06<00:00,  1.12it/s]\n",
      "100%|███████████████████████████████████████| 7813/7813 [33:49<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 0 as checkpoint_0_1.743630702907841.pth.tar\n",
      "Training loss:  1.9085582312057494\n",
      "Validation loss:  1.743630702907841\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjUAAAGdCAYAAADqsoKGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL+lJREFUeJzt3Ql8VOUV9/ETEhLZkhC2JJqwKgoKWpRFtAiksomktYiUJQiCKIoCUuQtiyKKIK0rYqtgpKIoi7iBVFFAdqFGBZQCBkggxIqQEJaAcN/PefreeWdCEjKQCfDM7/v53MaZe2e7meb+Oc957g1xHMcRAACAi1y58/0GAAAASgOhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABghTAJEqdOnZK9e/dKlSpVJCQk5Hy/HQAAUAJ6juBDhw5JfHy8lCtXfC0maEKNBpqEhITz/TYAAMBZyMjIkMsuu6zYbYIm1GiFxt0pkZGR5/vtAACAEsjNzTVFCfc4XpygCTXukJMGGkINAAAXl5K0jtAoDAAArECoAQAAViDUAAAAKwRNTw0AoHSdPHlSTpw4wW7FOStfvryEhoae8/MQagAAfsvLy5PMzExzDhGgNJqAdbp25cqVz+l5CDUAAL8rNBpoKlasKDVq1OCEpjgnGoz/+9//mu/U5Zdffk4VG0INAMAvOuSkByINNBUqVGDv4Zzpd2nnzp3mu3UuoYZGYQDAWeGSM7jQvkuEGgAAYAVCDQAAsAKhBgCAs1SnTh157rnnSrz9smXLzFDLwYMHA7rPU1NTJTo6WoINoQYAYD0NEsUtjz322Fk971dffSWDBg0q8fY33nijZGVlSVRU1Fm9HorH7CcAgPU0SLjeeecdGTdunGzdutVzn/f5UXRml05bDwsLK9GsHX+Eh4dLbGysX49ByVGpAQCcEw0BR47/el6Wkp78T4OEu2iVRKsz7u0ffvhBqlSpIosXL5ZmzZpJRESErFy5Unbs2CHdunWTWrVqmdBzww03yGeffVbs8JM+72uvvSa///3vzXl89LwrH3zwQZHDT+4w0ZIlS+Sqq64yr9OxY0efEPbrr7/K0KFDzXbVqlWTUaNGSUpKiiQnJ/v1e5o+fbrUr1/fBKuGDRvKP//5T5/foVarEhMTzeePj483r+l6+eWXzWe55JJLzP744x//KBciKjUAgHNy9MRJaTRuyXnZi1smdJCK4aVzKHv00Udl6tSpUq9ePalatapkZGRI586d5cknnzQH+lmzZknXrl1NhUcP/kV5/PHHZcqUKfLMM8/Iiy++KL169ZJdu3ZJTExModsfOXLEvK6GjHLlyknv3r3lkUcekdmzZ5v1kydPNv/9+uuvm+Dz/PPPy8KFC6Vt27Yl/mzvvfeePPTQQyaAJSUlyUcffSR33323OYuvPs/8+fPl2WeflTlz5kjjxo1l37598s0335jHbtiwwQQcfX86fPbLL7/Il19+KRciQg0AACIyYcIE+d3vfufZFxpCmjZt6rn9xBNPmHCglZcHHnigyH3Wr18/6dmzp/nvp556Sl544QVZv369qcAURk8498orr5gqitLn1vfi0mA0evRoU/1RL730kixatMiv39nUqVPN+7r//vvN7eHDh8vatWvN/Rpqdu/ebapWGnj0Okwa2po3b2621XWVKlWS2267zVS0ateuLdddd90F+Z0h1AAAzkmF8qGmYnK+Xru0XH/99add30qHZD7++GMzHKTDQEePHjUH+eI0adLE898aBiIjI+Wnn34qcnsdpnIDjYqLi/Nsn5OTI9nZ2Z6AofSMuzpMdurUqRJ/tu+///60hubWrVubqo/q3r27qeJolUrDl1aotCqlfUUa9DTIuOt0cYfXLjT01AAAzon2iOgQ0PlYSvOsxhpAvOkQkFZmtNqiwy1paWlyzTXXyPHjx4t9Hq10FNw/xQWQwrYv6wuFJiQkmGE17Z3RS19oRee3v/2tqSJpdebf//63vP322yZwaZO1VrACPS39bBBqAAAoxKpVq8yQjVYlNMzo8Ixen6gsaVOzNubq1HGXzszSkOGPq666ynweb3q7UaNGntsaZrQ6o8Nl2tC8Zs0a+e6778w6rdjo0JT2Cn377bdmP3z++edyoWH4CQCAQuhsnwULFpgDvVZPxo4d69eQT2l58MEHZdKkSdKgQQO58sorTY/NgQMH/KpSjRw5Uu68807TC6Ph5MMPPzSfzZ3NpbOwNCy1aNHCDCu9+eabJuTosJM2Ff/444+mcqMN1NrPo/tBZ1BdaAg1AAAU4m9/+5v079/fzPipXr26mUqdm5tb5vtKX1dnI/Xt29f002hvTIcOHfy6mnVycrLpn9HGYJ0FVbduXTOb6pZbbjHrdbr4008/bRqINdxoZUqDj04h13UagLS/6NixYybs6VCUzpK60IQ4ZT1wd57oF1HLeNp0pU1bAICzowe29PR0c2DU85agbGmVRIeTtPKiM7Js/07l+nH8plIDAMAFTM9x869//UvatGkj+fn5Zkq3BoA//elP5/utXXBoFAYA4AKmJ+TTnhc9o7FOw9bmXe2F0WoNfFGpAQDgAqbTrQvOXELhqNQAAAArEGoAAEBwhpoVK1aYOft6BU+dI68X1TqTadOmmbE/nfOu89r1omAFzZ0718y/165nnUpW8LoWOklLz2KoZzPU59F59tu2bfP37QMAAEv5HWoOHz5sTo+sQaWklzrXC3Hp/PbNmzebq5cOGTLEzH93rV692lz8a8CAAfL111+b+fS6bNq0ybONnsVQz3KoF/1at26dOZ21ztPXaWAAAADndJ4ardTodTE0gBRFT1qk3dp6CXbXiBEjTDBZuXKlud2jRw8TlvSsha6WLVvKtddea0KMvkWtDOnj9FocSuer66mjtSP8rrvuOuN75Tw1AFA6OE8NLtTz1AS8p0bn1Bd8gzp8pJdh1wtlKb2+hA4nedMqjN6v9IPq2RS9t9EPqKdzdrcp7HV1R3gvAACcCz0D78MPP+y5XadOHXN16+KUtFXjTErreYqjoypaULhYBTzUaDh57bXXZOPGjabismHDBnNbA83PP/9sttHAolUXb3pb73fXu/cVtU1Bep0MDT7uolPiAADBSXtBO3bsWOg6vQK3Bga9UKO/9EKTetmCsggWWVlZ0qlTp1J9LdsEPNToBcD0l6DDSXp59W7duklKSsr/Xrxc4F5e+3i0VOUuGRkZAXstAMCFTXs2P/30U8nMzDxtnV4D6frrr5cmTZr4/bw1atQwF4AsC3qV8IiIiDJ5rYtVwEONDjXNnDlTjhw5Yi5Vvnv3blOuq1KlivkyuL+o7Oxsn8fpbb3fXe/eV9Q2BekvXsfevBcAQHC67bbbzDFH+zC95eXlmdm3Gnr2799vJq1ceumlJqjoTFy9cGNxCg4/6axcvZq1tl00atTIBKnCLlB5xRVXmNeoV6+e+ce/246h708n1HzzzTemeqSL+54LDj/pmYXbtWtnjrN64clBgwaZz+Pq16+f6XnVi1jqzGHdRifquK9V0utMTZgwQS677DJzXNUK0ieffOJZf/z4cXnggQfM8+tn1qt660iJ0tEZrTolJiaax2pv7NChQ8WKMwprlUZ3ipozZ475grmVmlatWsnSpUt9xin1i6D3K20c0vCi27glOe2R0Wbj++67r6w+AgCgMDrf5MSR87NvylfUo/0ZNwsLCzNXudaA8Je//MUEBKWBRq9KrWFGA0GzZs1M6NB/CH/88cfSp08fqV+/vjRv3rxEAeAPf/iDaY3Q45OOEngf11z6j3p9H3qQ12AycOBAc9+f//xnM3FGZ/5qcNBLIShtoShIJ9doe4ceJ3UI7KeffpJ77rnHBAzv4PbFF1+YwKE/t2/fbp5fj6P6miWhV/b+61//Kn//+9/luuuuM0WK22+/3cxm1qt166zkDz74QN59910TXnRUxB0ZmT9/vjz77LPmmK9X9NZ2EQ1rF1So0V+67hiXNvGmpaVJTEyM+UA67LNnzx7PuWj+85//mKZgbeo9cOCAuZS7/sLeeOMNz3PoZdD1Ql2647p06WJ2gPbe/OMf/zDr9cunX4yJEyeanaghR5OtfiGKm3kFACgDGmieij8/u/r/7BUJr1SiTfv3729m4i5fvtw0/LpDT3fccYen/9KdYasefPBBWbJkiTlglyTUaAj54YcfzGP0+KSeeuqp0/pgxowZ41Pp0dfU456GGq26VK5c2YSwokYi1FtvvWVmDOmxVk9xol566SXTOzR58mRPD2rVqlXN/aGhoeZccHqM1QJBSUONVnk05LmzjPW5NSBpdUpP7aKjL3pcvummm8yxWis1Ll2nn0En+WhhQzNCSfZjmQ4/adjQtKaLGj58uPlvPTGe28ikH8SlCVjDip7b5ne/+535Jeh5afQX6T3tW39BGmJ0u3nz5pkS29VXX+3ZRn/Z+gXT8ppe1EvDlSZZLnsPACgJPajr8UarDUr/ga5Nwjr05B6vnnjiCTPspP9Q13ChAcX7mFac77//3kxKcQONckccvL3zzjvmVCd6wNfX0JBT0tfwfi09XrqBRrVu3dpUi7Zu3eq5TyskGmhcWrXRqk5J6IjI3r17zfN609v6+u4QlxY29MS6OrSkVxN3de/eXY4ePWqG2DRE6Slgfv31V7mgKjWabos7tU3B8Uo9k7CeUO9M9MPrUhRNgDqupwsA4AKiQ0BaMTlfr+0HDTD6D2StMmiVRoeWdKRAaRVHh1u0CqHBRgODjhJo30hp0dOQ9OrVy/TN6PCRVoe0SqP/+A+E8uXLn3Ys1eBTWn7zm9+YEZvFixebStWdd95pKjNanNCApwFL79eWkvvvv99TKSv4vkoL134CAJwb7U/RIaDzsZSgn8abHnS1n1NHB3ToRoek3P4avRK2ztDt3bu3qYJohUFbKEpK/xGv/SQ6YuFau3atzzY6UqFDNNrXozOudOhm165dPtuEh4ebqtGZXkv7U7S3xrVq1Srz2bRqUhq0r0irTgWvEK63tQnaezvt1Xn11VdNFUp7aX755RezTofTdEhMe2+WLVtmQp32EV30jcIAAJxvOtyjB2Dt/9ThFR0+cWnA0AqDBg/tRdEeUJ1l630AL45WKHRWk562RCsS+vwaXrzpa+hQk1ZntJVCm5F1WMabtme4/ao6wUabiAtO5dZqz/jx481r6Qyj//73v6YCpY3NBc/pdi5GjhxpXkcrWtpgrNUtfV+zZ88263Uf6ZCWtqFooNLGax1Wi46ONiM3Gs60p1Zner355psm5Hj33ZQ2KjUAgKCiQ1A6cUWHf7z7X7S3RYdT9H5ttdCDsz+TUfSgrgFF+0i0IVZnIz355JM+2+jMoWHDhplZShoSNEDpxBdv2risJwps27atmYZe2LRyDQna76MVEQ1Hf/zjH6V9+/amKbg0aZ+M9s7qZYp0SE57WXW2k4YzpYFLr82oVSd9H3rqFr0gte4LDTZavdEeHD0HkA5D6XUfdWr5BXntp4sJ134CgNLBtZ8QtNd+AgAAKAuEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQCclSCZPIuL6LtEqAEA+MW9llBpXj4Awe34//sueV+n6mxwRmEAgH8HjrAwc/I3PYutXsNHT7QGnC29FpV+l/Q7pd+tc0GoAQD4Ra+VpKfG15OlFbxuEXA2NBgnJiZ6rsN1tgg1AAC/6UUX9VT5DEGhtL5PpVHxI9QAAM6KHoQKntIeOJ8YCAUAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAMEZalasWCFdu3aV+Ph4CQkJkYULF57xMbNnz5amTZtKxYoVJS4uTvr37y/79+/3rL/lllvMcxVcunTp4tmmX79+p63v2LGjv28fAABYyu9Qc/jwYRNQpk2bVqLtV61aJX379pUBAwbI5s2bZe7cubJ+/XoZOHCgZ5sFCxZIVlaWZ9m0aZOEhoZK9+7dfZ5LQ4z3dm+//ba/bx8AAFgqzN8HdOrUySwltWbNGqlTp44MHTrU3K5bt67ce++9MnnyZM82MTExPo+ZM2eOqeoUDDURERESGxvr71sGAABBIOA9Na1atZKMjAxZtGiROI4j2dnZMm/ePOncuXORj5kxY4bcddddUqlSJZ/7ly1bJjVr1pSGDRvKfffd5zOEVVB+fr7k5ub6LAAAwF4BDzWtW7c2PTU9evSQ8PBwU2mJiooqcvhKh6Z0+Omee+45behp1qxZsnTpUlPlWb58uakYnTx5stDnmTRpknkdd0lISAjI5wMAABeGEEfLJ2f74JAQee+99yQ5ObnIbbZs2SJJSUkybNgw6dChg+mFGTlypNxwww2mIlOQDk3pkNW3335b7Gv/+OOPUr9+ffnss8+kffv2hVZqdHFppUaDTU5OjkRGRvr9WQEAQNnT47cWJ0py/Pa7p8ZfWjHRao0GGdWkSRMzrHTzzTfLxIkTzWwo7yZk7aeZMGHCGZ+3Xr16Ur16ddm+fXuhoUb7b3QBAADBIeDDT0eOHJFy5XxfRmc2qYJFIp0ZpdWV3r17n/F5MzMzTU+NdygCAADBy+9Qk5eXJ2lpaWZR6enp5r93795tbo8ePdpM4XbpOW10yvb06dPNkJFO8daZUM2bNzfnuvGmw1E6lFWtWrXTXlMrPWvXrpWdO3eavppu3bpJgwYNzJAWAACA38NPGzZskLZt23puDx8+3PxMSUmR1NRU0zPjBhz3pHmHDh2Sl156SUaMGCHR0dHSrl07nyndauvWrbJy5Ur517/+ddpramVHe2zeeOMNOXjwoAlDt956qzzxxBMMMQEAgHNvFLa10QgAAFx8x2+u/QQAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAAAEZ6hZsWKFdO3aVeLj4yUkJEQWLlx4xsfMnj1bmjZtKhUrVpS4uDjp37+/7N+/37M+NTXVPJf3cskll/g8h+M4Mm7cOPP4ChUqSFJSkmzbts3ftw8AACzld6g5fPiwCSjTpk0r0farVq2Svn37yoABA2Tz5s0yd+5cWb9+vQwcONBnu8jISMnKyvIsu3bt8lk/ZcoUeeGFF+SVV16RdevWSaVKlaRDhw5y7Ngxfz8CAACwUJi/D+jUqZNZSmrNmjVSp04dGTp0qLldt25duffee2Xy5Mk+22l1JjY2ttDn0CrNc889J2PGjJFu3bqZ+2bNmiW1atUylaK77rrL348BAAAsE/CemlatWklGRoYsWrTIhJPs7GyZN2+edO7c2We7vLw8qV27tiQkJJjgolUdV3p6uuzbt88MObmioqKkRYsWJjQVJj8/X3Jzc30WAABgr4CHmtatW5uemh49ekh4eLipxmgg8R6+atiwocycOVPef/99efPNN+XUqVNy4403SmZmplmvgUZpZcab3nbXFTRp0iTzOu6iYQkAANgr4KFmy5Yt8tBDD5km340bN8onn3wiO3fulMGDB/tUc7Tv5tprr5U2bdrIggULpEaNGvL3v//9rF939OjRkpOT41m0WgQAAOzld0+Nv7RiotWakSNHmttNmjQxTb4333yzTJw40cxmKqh8+fJy3XXXyfbt281tt9dGh668t9fbGoQKExERYRYAABAcAl6pOXLkiJQr5/syoaGh5qf22BTm5MmT8t1333kCjDYXa7BZunSpZxvtkdFZUFrlAQAA8LtSow29bgXFbeJNS0uTmJgYSUxMNMM+e/bsMbOTlJ7TRqdvT58+3UzB1unaDz/8sDRv3tyc60ZNmDBBWrZsKQ0aNJCDBw/KM888Y6Z033PPPZ6ZUfoYrexcfvnlJuSMHTvWPD45OZnfIgAA8D/UbNiwQdq2beu5PXz4cPMzJSXFnERPQ8vu3bs96/v16yeHDh2Sl156SUaMGCHR0dHSrl07nyndBw4cMMFHm36rVq0qzZo1k9WrV0ujRo082/z5z38258gZNGiQCT433XST6c8peJI+AAAQnEKcosaALKPDVToLSpuG9UR/AADAruM3134CAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAAgjPUrFixQrp27Srx8fESEhIiCxcuPONjZs+eLU2bNpWKFStKXFyc9O/fX/bv3+9Z/+qrr8rNN98sVatWNUtSUpKsX7/e5zn69etnXs976dixo79vHwAAWMrvUHP48GETUKZNm1ai7VetWiV9+/aVAQMGyObNm2Xu3LkmsAwcONCzzbJly6Rnz57yxRdfyJo1ayQhIUFuvfVW2bNnj89zaYjJysryLG+//ba/bx8AAFgqzN8HdOrUySwlpSGlTp06MnToUHO7bt26cu+998rkyZN9KjneXnvtNZk/f74sXbrUBCJXRESExMbG+vuWAQBAEAh4T02rVq0kIyNDFi1aJI7jSHZ2tsybN086d+5c5GOOHDkiJ06ckJiYGJ/7taJTs2ZNadiwodx3330+Q1gF5efnS25urs8CAADsFfBQ07p1a1OJ6dGjh4SHh5tKS1RUVLHDV6NGjTI9O9pb4z30NGvWLFO90SrP8uXLTcXo5MmThT7HpEmTzOu4iw5pAQAAe4U4Wj452weHhMh7770nycnJRW6zZcsWE06GDRsmHTp0ML0wI0eOlBtuuEFmzJhx2vZPP/20TJkyxVRlmjRpUuTz/vjjj1K/fn357LPPpH379oVWanRxaaVGg01OTo5ERkae1ecFAABlS4/fWpwoyfHb754af2nFRKs1GmSUBpVKlSqZ2U4TJ040s6FcU6dONaFGg0pxgUbVq1dPqlevLtu3by801Gj/jS4AACA4BDzUaH9MWJjvy4SGhpqf3kUirc48+eSTsmTJErn++uvP+LyZmZmmp8Y7FAEAgODld09NXl6epKWlmUWlp6eb/969e7e5PXr0aJ8ZS3pOmwULFsj06dPNkJFO8daZUM2bNzd9M0p7ZMaOHSszZ840M6X27dtnFn0t9zW10rN27VrZuXOn6avp1q2bNGjQwAxpAQAA+N1To70ubdu2Pe3+lJQUSU1NNSfJ0+Ch27lefPFFeeWVV0wAio6Olnbt2pkgc+mll5r1GmR27dp12nOOHz9eHnvsMTl69Kjp2/n666/l4MGDJgzpeWyeeOIJqVWrVqmPyQEAgAuDP8fvc2oUvpgQagAAsPv4zbWfAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAMEZalasWCFdu3aV+Ph4CQkJkYULF57xMbNnz5amTZtKxYoVJS4uTvr37y/79+/32Wbu3Lly5ZVXyiWXXCLXXHONLFq0yGe94zgybtw48/gKFSpIUlKSbNu2zd+3DwAALOV3qDl8+LAJKNOmTSvR9qtWrZK+ffvKgAEDZPPmzSa8rF+/XgYOHOjZZvXq1dKzZ0+zzddffy3Jyclm2bRpk2ebKVOmyAsvvCCvvPKKrFu3TipVqiQdOnSQY8eO+fsRAACAhUIcLYGc7YNDQuS9994zAaQoU6dOlenTp8uOHTs897344osyefJkyczMNLd79OhhwtJHH33k2aZly5Zy7bXXmhCjb1ErQyNGjJBHHnnErM/JyZFatWpJamqq3HXXXWd8r7m5uRIVFWUeFxkZebYfGQAAlCF/jt8B76lp1aqVZGRkmOEkDSfZ2dkyb9486dy5s2ebNWvWmOEkb1qF0ftVenq67Nu3z2cb/YAtWrTwbAMAAIJbwENN69atTU+NVmPCw8MlNjbWBBLv4SsNLFp18aa39X53vXtfUdsUlJ+fb9Kd9wIAAOwV8FCzZcsWeeihh0yT78aNG+WTTz6RnTt3yuDBgwP6upMmTTLhyV0SEhIC+noAAMDyUKPhQqs1I0eOlCZNmphhpZdffllmzpwpWVlZZhut3uiwlDe9rfe76937itqmoNGjR5vxN3fRITAAAGCvgIeaI0eOSLlyvi8TGhpqfro9ytp3s3TpUp9tPv30U3O/qlu3rgkv3tvocJLOgnK3KSgiIsI0FHkvAADAXmH+PiAvL0+2b9/uua1NvGlpaRITEyOJiYmmQrJnzx6ZNWuWWa/ntNHp2zoDSqs0Wp15+OGHpXnz5mZGk9LhqTZt2shf//pX6dKli8yZM0c2bNgg//jHPzyzrPQxEydOlMsvv9yEnLFjx5rHFzfzCgAABBHHT1988YWWV05bUlJSzHr92aZNG5/HvPDCC06jRo2cChUqOHFxcU6vXr2czMxMn23effdd54orrnDCw8Odxo0bOx9//LHP+lOnTjljx451atWq5URERDjt27d3tm7dWuL3nZOTY96n/gQAABcHf47f53SemosJ56kBAODic0GdpwYAAKAsEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAwRlqVqxYIV27dpX4+HgJCQmRhQsXFrt9v379zHYFl8aNG3u2qVOnTqHbDBkyxLPNLbfcctr6wYMH+/v2AQCApfwONYcPH5amTZvKtGnTSrT9888/L1lZWZ4lIyNDYmJipHv37p5tvvrqK59tPv30U3O/9zZq4MCBPttNmTLF37cPAAAsFebvAzp16mSWkoqKijKLSys7Bw4ckLvvvttzX40aNXwe8/TTT0v9+vWlTZs2PvdXrFhRYmNj/X3LAAAgCJR5T82MGTMkKSlJateuXej648ePy5tvvin9+/c3Q0zeZs+eLdWrV5err75aRo8eLUeOHCmjdw0AAKyr1JyLvXv3yuLFi+Wtt94qchut5Bw8eND04nj705/+ZIKQ9vJ8++23MmrUKNm6dassWLCg0OfJz883iys3N7cUPwkAAAjqUPPGG29IdHS0JCcnF1vJ0eEtDS/eBg0a5Pnva665RuLi4qR9+/ayY8cOM1RV0KRJk+Txxx8v5U8AAAAk2IefHMeRmTNnSp8+fSQ8PLzQbXbt2iWfffaZ3HPPPWd8vhYtWpif27dvL3S9Dk/l5OR4Fm1QBgAA9iqzSs3y5ctNABkwYECR27z++utSs2ZN6dKlyxmfLy0tzfzUik1hIiIizAIAAIKD36EmLy/PpzqSnp5uAoZO005MTDQVkj179sisWbNOG1bS6oo2+Rbm1KlTJtSkpKRIWJjv29IhJu3D6dy5s1SrVs301AwbNkx++9vfSpMmTfz9CAAAwEJ+h5oNGzZI27ZtPbeHDx9ufmoYSU1NNeeP2b17t89jdPhn/vz55pw1RdFhJ32cznoqSIerdP1zzz1nzpOTkJAgd9xxh4wZM8bftw8AACwV4mizSxDQ2U96vhwNWJGRkef77QAAgFI+fnPtJwAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAAMAKhBoAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArEGoAAIAVCDUAACA4Q82KFSuka9euEh8fLyEhIbJw4cJit+/Xr5/ZruDSuHFjzzaPPfbYaeuvvPJKn+c5duyYDBkyRKpVqyaVK1eWO+64Q7Kzs/19+wAAwFJ+h5rDhw9L06ZNZdq0aSXa/vnnn5esrCzPkpGRITExMdK9e3ef7TTkeG+3cuVKn/XDhg2TDz/8UObOnSvLly+XvXv3yh/+8Ad/3z4AALBUmL8P6NSpk1lKKioqyiwurewcOHBA7r77bt83EhYmsbGxhT5HTk6OzJgxQ9566y1p166due/111+Xq666StauXSstW7b092MAAADLlHlPjYaTpKQkqV27ts/927ZtM0Na9erVk169esnu3bs96zZu3CgnTpwwj3Pp8FRiYqKsWbOmTN8/AACwpFJzLnTIaPHixabi4q1FixaSmpoqDRs2NENPjz/+uNx8882yadMmqVKliuzbt0/Cw8MlOjra53G1atUy6wqTn59vFldubm6APhUAAAi6UPPGG2+YYJKcnOxzv/dwVpMmTUzI0UrOu+++KwMGDDir15o0aZIJRwAAIDiU2fCT4zgyc+ZM6dOnj6m6FEeDzxVXXCHbt283t7XX5vjx43Lw4EGf7XT2U1F9OKNHjza9OO6iDcoAAMBeZRZqdMaShpSSVF7y8vJkx44dEhcXZ243a9ZMypcvL0uXLvVss3XrVtN306pVq0KfIyIiQiIjI30WAABgL7+HnzRwuBUUlZ6eLmlpaWaatjbuaoVkz549MmvWrNMahHVY6eqrrz7tOR955BFz7hsdctK+m/Hjx0toaKj07NnTrNfZUxqGhg8fbl5HA8qDDz5oAg0znwAAwFmFmg0bNkjbtm09tzVoqJSUFNPsq42+3jOXlA7/zJ8/35yzpjCZmZkmwOzfv19q1KghN910k5mqrf/tevbZZ6VcuXLmpHvaANyhQwd5+eWX+S0CAAAjxNFmlyCgs5+04qMBi6EoAADsO35z7ScAAGAFQg0AALACoQYAAFiBUAMAAKxAqAEAAFYg1AAAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAABgBUINAACwAqEGAABYgVADAACsQKgBAABWINQAAAArhEmQcBzH/MzNzT3fbwUAAJSQe9x2j+PFCZpQc+jQIfMzISHhfL8VAABwFsfxqKioYrcJcUoSfSxw6tQp2bt3r1SpUkVCQkIk2Gny1YCXkZEhkZGR5/vtWIv9zH62Dd9p9nNZ05iigSY+Pl7KlSu+ayZoKjW6Iy677LLz/TYuOBpoCDXsZ1vwfWZf24bv9P+cqULjolEYAABYgVADAACsQKgJUhERETJ+/HjzE+znix3fZ/a1bfhOn52gaRQGAAB2o1IDAACsQKgBAABWINQAAAArEGoAAIAVCDWW+uWXX6RXr17mxE3R0dEyYMAAycvLK/Yxx44dkyFDhki1atWkcuXKcscdd0h2dnah2+7fv9+czFDPznzw4EEJZoHY199884307NnTnPW5QoUKctVVV8nzzz8vwWTatGlSp04dueSSS6RFixayfv36YrefO3euXHnllWb7a665RhYtWuSzXudEjBs3TuLi4sw+TUpKkm3btkmwK839fOLECRk1apS5v1KlSuYMsH379jVncw92pf199jZ48GDzt/i5554LwDu/yOjsJ9inY8eOTtOmTZ21a9c6X375pdOgQQOnZ8+exT5m8ODBTkJCgrN06VJnw4YNTsuWLZ0bb7yx0G27devmdOrUSWfOOQcOHHCCWSD29YwZM5yhQ4c6y5Ytc3bs2OH885//dCpUqOC8+OKLTjCYM2eOEx4e7sycOdPZvHmzM3DgQCc6OtrJzs4udPtVq1Y5oaGhzpQpU5wtW7Y4Y8aMccqXL+989913nm2efvppJyoqylm4cKHzzTffOLfffrtTt25d5+jRo06wKu39fPDgQScpKcl55513nB9++MFZs2aN07x5c6dZs2ZOMAvE99m1YMEC8/cnPj7eefbZZ51gR6ixkP6fQMPGV1995blv8eLFTkhIiLNnz55CH6N/jPT/NHPnzvXc9/3335vn0T9M3l5++WWnTZs25oAc7KEm0Pva2/333++0bdvWCQZ6IBwyZIjn9smTJ80f7UmTJhW6/Z133ul06dLF574WLVo49957r/nvU6dOObGxsc4zzzzj83uIiIhw3n77bSdYlfZ+Lsz69evNd3vXrl1OsArUfs7MzHQuvfRSZ9OmTU7t2rUJNY7jMPxkoTVr1phhkOuvv95zn5ba9fpX69atK/QxGzduNKVj3c6lpc/ExETzfK4tW7bIhAkTZNasWWe8sFgwCOS+LignJ0diYmLEdsePHzf7yHv/6P7U20XtH73fe3vVoUMHz/bp6emyb98+n230WjI6DFDcPrdZIPZzUd9bHRrR/58Eo0DtZ71Ic58+fWTkyJHSuHHjAH6CiwtHJQvpH++aNWv63BcWFmYOiLquqMeEh4ef9oenVq1ansfk5+ebPo9nnnnGHIARuH1d0OrVq+Wdd96RQYMGWb/bf/75Zzl58qTZHyXdP3p/cdu7P/15TtsFYj8X1jumPTb6dyNYL5wbqP08efJk87dm6NChAXrnFydCzUXk0UcfNf/iKW754YcfAvb6o0ePNg2rvXv3Ftud733tbdOmTdKtWzdzWYtbb721TF4TOFdajbzzzjtNg/b06dPZoaVIKz86cSA1NdX8LcL/F+b137jAjRgxQvr161fsNvXq1ZPY2Fj56aeffO7/9ddfzSwdXVcYvV/LpDqTybuCoDNy3Md8/vnn8t1338m8efPMbfcKG9WrV5e//OUv8vjjj4stzve+9h7ua9++vanQjBkzRoKBfp9CQ0NPm3lX2P5x6f3Fbe/+1Pt09pP3Ntdee60Eo0Ds54KBZteuXebvRrBWaQK1n7/88kvzd8e7Yq7VoBEjRpgZUDt37pSgFZi2KFwIzas6q8a1ZMmSEjWvzps3z3Ofzl7wbl7dvn276b53F+3k1/WrV68usovfdoHa10qb/2rWrOmMHDnSCcbGygceeMCnsVIbIotrrLztttt87mvVqtVpjcJTp071rM/JyaFRuJT3szp+/LiTnJzsNG7c2Pnpp5/8/t3bqLT3888//+zzt1gXbTweNWqU+VsSzAg1Fk8zvu6665x169Y5K1eudC6//HKfacbaNd+wYUOz3nuacWJiovP555+bg7T+n0iXonzxxRdBP/spUPta/0jVqFHD6d27t5OVleVZguUgoVNgdWZSamqqCY6DBg0yU2D37dtn1vfp08d59NFHfabAhoWFmdCiM8nGjx9f6JRufY7333/f+fbbb81pCZjSXbr7WQONTpW/7LLLnLS0NJ/vbn5+vhOsAvF9LojZT/9DqLHU/v37zYG1cuXKTmRkpHP33Xc7hw4d8qxPT083gUSDiUvP16HThqtWrepUrFjR+f3vf2/+GBWFUBO4fa1/xPQxBRf9wxUs9Jw8Gvz0/B76L109D5BLTymQkpLis/27777rXHHFFWZ7rRJ8/PHHPuu1WjN27FinVq1a5gDTvn17Z+vWrU6wK8397H7XC1u8v//BqLS/zwURav4nRP/nfA+BAQAAnCtmPwEAACsQagAAgBUINQAAwAqEGgAAYAVCDQAAsAKhBgAAWIFQAwAArECoAQAAViDUAAAAKxBqAACAFQg1AADACoQaAAAgNvi/Jgvt6Mxt7hcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(current_epoch, epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "\n",
    "    vit.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        # Teacher forcing\n",
    "        input_data = data[:, :-1].long().to(device)\n",
    "        target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction, load_balancing_loss = vit(input_data, [None] * input_data.shape[0])\n",
    "\n",
    "        # Change shape for loss calculation\n",
    "        prediction = prediction.view(-1, prediction.shape[-1])\n",
    "        target_data = target_data.reshape(-1)\n",
    "\n",
    "        loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    vit.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(validation_loader):\n",
    "            # Teacher forcing\n",
    "            input_data = data[:, :-1].long().to(device)\n",
    "            target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            prediction, load_balancing_loss = vit(input_data, [None] * input_data.shape[0])\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, prediction.shape[-1])\n",
    "            target_data = target_data.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "\n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "\n",
    "        loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(vit, optimizer, epoch, loss_valid[-1])\n",
    "\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    print(\"Training loss: \", loss_train[-1])\n",
    "    print(\"Validation loss: \", loss_valid[-1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7073d-1eec-47ae-819f-2ea4d52df671",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2fcd889-f1c8-46d7-8638-1e709083489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b692d-0473-409f-83df-8e9b69084a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b46013d-1085-4d93-95b1-8ed02d8d9d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv 是一个开源的例子，一个包含一个函数的特征。\n"
     ]
    }
   ],
   "source": [
    "sentence = \"arXiv is an open-access\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "if tokenized_sentence[-1] == 2:\n",
    "    tokenized_sentence = tokenized_sentence[:-1]\n",
    "vit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_context_length): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction, _ = vit(tokenized_sentence_tensor, [None])\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "        \n",
    "tokens = tokenizer.decode(tokenized_sentence, skip_special_tokens=True)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
