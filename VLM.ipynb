{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c799cdf9-7e32-460b-9f0d-5b64b4839ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danjietang/Documents/Github/GemmaLLM/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109184df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import math\n",
    "from transformers import AutoTokenizer, CLIPVisionModel, CLIPImageProcessor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2660dd2d-868b-44d9-8842-f652c749286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_context_length = 51\n",
    "device = \"mps\"\n",
    "epochs = 1\n",
    "batch_size = 768\n",
    "validation_batch_size = 768\n",
    "weight_decay = 1e-3\n",
    "lr = 1e-3\n",
    "num_layer = 3\n",
    "head_dim = 64\n",
    "projection_dim = 512\n",
    "expansion_factor = 16\n",
    "checkpoint_filepath = \"\"\n",
    "q_head = 8\n",
    "kv_head = 4\n",
    "training_data_path = \"llama2_wiki_50_train.npy\"\n",
    "eval_data_path = \"llama2_wiki_50_eval.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99869320-6624-4b85-9bf1-c982adc6a947",
   "metadata": {},
   "source": [
    "# Load llama2 token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0878240-ec08-4df2-a4ce-a03793b4f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097cdf0-fc3a-4089-a0ce-53a968012f1d",
   "metadata": {},
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b66a5b-1c5e-4b96-85e2-62cffd1224e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # Create memory-mapped array\n",
    "        self.mmap_data = np.load(filename, mmap_mode='r')\n",
    "        self.length = self.mmap_data.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.mmap_data[idx])\n",
    "\n",
    "training_dataset = Dataset(training_data_path)\n",
    "eval_dataset = Dataset(eval_data_path)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(eval_dataset, batch_size=validation_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6301f17-1ea0-4394-be58-14ad70344f13",
   "metadata": {},
   "source": [
    "# LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d780b67b-5d0b-479e-af1f-4ffdda4ab810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROPEEmbedding(nn.Module):\n",
    "    def __init__(self, max_context_length: int, head_dim: int = 64, theta: int = 10000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.create_embedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_embedding(max_context_length: int, head_dim: int = 64, theta: int = 10000) -> torch.Tensor:\n",
    "        # Angles\n",
    "        tensor = torch.arange(0, head_dim // 2)\n",
    "        tensor = torch.repeat_interleave(tensor, 2)\n",
    "        tensor = -tensor * 2 / head_dim\n",
    "        tensor = torch.pow(theta, tensor)\n",
    "\n",
    "        index = torch.arange(max_context_length).float() # This is the m in the formula\n",
    "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
    "\n",
    "        cos_matrix = tensor.cos()\n",
    "        sin_matrix = tensor.sin()\n",
    "        sin_matrix[0::2, :] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
    "\n",
    "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
    "        pos_emb = pos_emb.transpose(1, 0)\n",
    "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    @staticmethod\n",
    "    def flip_for_sin(tensor: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, S, D = tensor.shape\n",
    "        # original_shape = tensor.shape\n",
    "        tensor = tensor.view(B, H, S, D//2, 2) # Get to pairs\n",
    "        # tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
    "        tensor = tensor[:, :, :, :, [1, 0]].contiguous() # Swap\n",
    "        # tensor = tensor[..., [1, 0]] \n",
    "        tensor = tensor.view(B, H, S, D) # Get back to original shape\n",
    "        # tensor = tensor.reshape(original_shape) \n",
    "        return tensor\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
    "\n",
    "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
    "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
    "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
    "        tensor = cos + sin\n",
    "        return tensor\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.embedding = embedding\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "\n",
    "        # LoRA\n",
    "        self.lora_scale = lora_alpha / lora_rank\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "        # Gated attention from Qwen paper\n",
    "        # Each gate gets one gate score as recommended in the paper\n",
    "        self.gate = nn.Linear(hidden_dim, q_head)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "        pre_norm_tensor = tensor # Used to calculate gate score\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Apply ROPE\n",
    "        query = self.embedding(query)\n",
    "        key = self.embedding(key)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Gated attention from Qwen paper\n",
    "        gate_score = self.gate(pre_norm_tensor)\n",
    "        gate_score = F.sigmoid(gate_score)\n",
    "        gate_score = gate_score.repeat_interleave(self.head_dim, dim=-1)\n",
    "        value = value * gate_score\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(value)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "        # LoRA\n",
    "        self.lora_scale = lora_alpha / lora_rank\n",
    "        self.lora_gate_and_up_a = nn.Linear(hidden_size, lora_rank)\n",
    "        self.lora_gate_and_up_b = nn.Linear(lora_rank, hidden_size * expansion_factor * 2)\n",
    "        self.lora_down_a = nn.Linear(hidden_size * expansion_factor, lora_rank)\n",
    "        self.lora_down_b = nn.Linear(lora_rank, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        gate_and_up = self.gate_and_up(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_gate_and_up_a(tensor)\n",
    "            lora_tensor = self.lora_gate_and_up_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            gate_and_up = gate_and_up + lora_tensor\n",
    "        gate, up = gate_and_up.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        down_tensor = self.down(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_down_a(tensor)\n",
    "            lora_tensor = self.lora_down_b(lora_tensor)\n",
    "            lora_tensor = lora_tensor * self.lora_scale\n",
    "            down_tensor = down_tensor + lora_tensor\n",
    "        return down_tensor\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1, lora_rank: int = 16, lora_alpha: int = 32, device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, lora_alpha=lora_alpha) for _ in range(num_experts)])\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).to(self.device)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i], fine_tuning) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i], fine_tuning) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(self.device) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(self.device)\n",
    "        flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(self.device)\n",
    "        flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "class LLMLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.mqa = Attention(hidden_dim, head_dim, q_head, kv_head, embedding, lora_rank=lora_rank, lora_alpha=lora_alpha)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, num_experts=num_experts, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, device=device)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank, lora_alpha=lora_alpha)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.mqa(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor, fine_tuning=fine_tuning)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor, fine_tuning=fine_tuning)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=self.device)# If not using MoE, load-balancing loss is zero\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 vocabulary_size: int,\n",
    "                 max_context_length: int,\n",
    "                 hidden_dim: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "        self.embedding = ROPEEmbedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(LLMLayer(hidden_dim, head_dim, q_head, kv_head, self.embedding, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, use_moe=use_moe, num_experts=num_experts, lora_rank=lora_rank, lora_alpha=lora_alpha, device=device))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, vocabulary_size)\n",
    "        self.device = device\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"pos_emb\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, causal_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            tensor, load_balancing_loss = layer(tensor, attention_mask=causal_mask, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Classification\n",
    "        tensor = self.output_norm(tensor)\n",
    "        tensor = self.classifier(tensor)\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class VLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 max_context_length: int,\n",
    "                 word_embeddings_tensor: str,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 projection_dim: int = None,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16,\n",
    "                 lora_alpha: int = 32,\n",
    "                 device: str = \"mps\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Right now this code is hard coded to use CLIP ViT\n",
    "        model_id = \"openai/clip-vit-large-patch14\"\n",
    "        self.vision_model = CLIPVisionModel.from_pretrained(model_id)\n",
    "        self.vision_processor = CLIPImageProcessor.from_pretrained(model_id)\n",
    "        image_dim = 1024\n",
    "        self.visual_seq_len = 1# Vision sequence length\n",
    "\n",
    "        # Load model token embeddings\n",
    "        self.word_embeddings_tensor = torch.load(word_embeddings_tensor)\n",
    "        zero_row = torch.zeros(1, self.word_embeddings_tensor.shape[1])\n",
    "        self.word_embeddings_tensor = torch.cat((self.word_embeddings_tensor, zero_row), dim=0).to(device)\n",
    "        self.vocabulary_size, text_dim = self.word_embeddings_tensor.shape\n",
    "        self.word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "        # The token that seperates image tokens from text tokens\n",
    "        if projection_dim:\n",
    "            self.seperation_token = nn.Parameter(torch.randn(1, projection_dim))\n",
    "        else:\n",
    "            self.seperation_token = nn.Parameter(torch.randn(1, text_dim))\n",
    "\n",
    "        # Potentially image&text token projection layer\n",
    "        if projection_dim:\n",
    "            self.image_projection = image_dim != projection_dim\n",
    "            self.text_projection = text_dim != projection_dim\n",
    "        else: # If no explicit projection_dim, use text token_dim\n",
    "            self.image_projection = image_dim != text_dim\n",
    "            self.text_projection = False\n",
    "        if projection_dim:\n",
    "            if self.image_projection:\n",
    "                self.image_token_projection = nn.Linear(image_dim, projection_dim)\n",
    "            if self.text_projection:\n",
    "                self.text_token_projection = nn.Linear(text_dim, projection_dim)\n",
    "        else: \n",
    "            if self.image_projection:\n",
    "                self.image_token_projection = nn.Linear(image_dim, text_dim)\n",
    "\n",
    "        # Create the LLM\n",
    "        self.llm = LLM(num_layer, self.vocabulary_size, max_context_length, projection_dim if projection_dim else text_dim, expansion_factor=expansion_factor, head_dim=head_dim, q_head=q_head, kv_head=kv_head, dropout_ratio=dropout_ratio, theta=theta, use_moe=use_moe, num_experts=num_experts, load_balancing_loss_weight=load_balancing_loss_weight, fine_tuning=fine_tuning, lora_rank=lora_rank, lora_alpha=lora_alpha, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, image_paths: list[str | None]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, text_seq_len = token_ids.shape\n",
    "        pad_amount = self.visual_seq_len + 1\n",
    "        multimodal_seq_len = text_seq_len + pad_amount\n",
    "        assert batch_size == len(image_paths), \"Mismatch between text and image inputs\"\n",
    "\n",
    "        # 1. Preallocate input data\n",
    "        multimodal_input_tensor = torch.empty(\n",
    "            (batch_size, multimodal_seq_len, self.llm.classifier.in_features), \n",
    "            device=self.device, \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # 2. Vectorized Text Embedding & Projection\n",
    "        # input_embeddings.shape = [batch, seq_len, emb_dim]\n",
    "        input_embeddings = self.word_embeddings_tensor[token_ids].float()\n",
    "        if self.text_projection:\n",
    "            input_embeddings = self.text_token_projection(input_embeddings)\n",
    "        \n",
    "        # Place text embeddings into the right side of the pre-allocated tensor\n",
    "        multimodal_input_tensor[:, pad_amount:, :] = input_embeddings\n",
    "\n",
    "        # 3. Collect images\n",
    "        has_image_mask = torch.tensor([p is not None for p in image_paths], device=self.device)\n",
    "        image_indices = torch.where(has_image_mask)[0]\n",
    "\n",
    "        # Prepare the padding embedding (for batches without images)\n",
    "        pad_token_emb = self.word_embeddings_tensor[-1:].float()\n",
    "        if self.text_projection:\n",
    "            pad_token_emb = self.text_token_projection(pad_token_emb)\n",
    "        \n",
    "        # Fill the left side with padding by default\n",
    "        multimodal_input_tensor[:, :pad_amount, :] = pad_token_emb.repeat(batch_size, pad_amount, 1)\n",
    "\n",
    "        if len(image_indices) > 0:\n",
    "            # Process only the actual images found\n",
    "            actual_images = [Image.open(image_paths[i]).convert(\"RGB\") for i in image_indices.tolist()]\n",
    "            vision_inputs = self.vision_processor(images=actual_images, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.vision_model(**vision_inputs)\n",
    "                visual_tokens = outputs.last_hidden_state[:, 0:self.visual_seq_len, :]\n",
    "            \n",
    "            if self.image_projection:\n",
    "                visual_tokens = self.image_token_projection(visual_tokens)\n",
    "                \n",
    "            # Scatter visual tokens and separation tokens into the multimodal tensor\n",
    "            multimodal_input_tensor[image_indices, :self.visual_seq_len, :] = visual_tokens\n",
    "            multimodal_input_tensor[image_indices, self.visual_seq_len, :] = self.seperation_token\n",
    "\n",
    "        # 4. 3D Causal Mask\n",
    "        causal_mask = torch.triu(torch.ones(multimodal_seq_len, multimodal_seq_len) * float('-inf'), diagonal=1).to(device)\n",
    "        causal_mask.requires_grad = False\n",
    "        \n",
    "        # 5. Forward to LLM\n",
    "        tensor, load_balancing_loss = self.llm(multimodal_input_tensor, causal_mask)\n",
    "        tensor = tensor[:, pad_amount:, :]\n",
    "        tensor = tensor.contiguous()\n",
    "    \n",
    "        return tensor, load_balancing_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f4c49-f6a1-4b7c-a028-16440d116502",
   "metadata": {},
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf49fd2e-f6d8-41e3-91a4-af480f249b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_to_save = model.module\n",
    "    else:\n",
    "        model_to_save = model\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_to_save.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'checkpoint_{epoch}_{loss}.pth.tar'\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f'Checkpoint saved at epoch {epoch} as {filename}')\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename) -> int:\n",
    "    checkpoint = torch.load(filename, weights_only=False)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Checkpoint loaded from epoch {epoch} with loss {loss}')\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b561ab-1778-4c6c-9e3d-c928ef711fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VLM(num_layer, max_context_length, 'word_embeddings_tensor_llama2.pt', projection_dim=projection_dim, expansion_factor=expansion_factor, use_moe=False, q_head=q_head, kv_head=kv_head, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a08e515-f4bd-4e99-a408-c16b09ee0760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 363815769 parameters.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "total_steps = epochs*len(training_loader)\n",
    "warmup_steps = int(total_steps * 0.01)\n",
    "\n",
    "# Warmup: LR linearly increases from 0 → base LR over warmup_steps\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_steps)\n",
    "\n",
    "# Cosine annealing: after warmup\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=(total_steps - warmup_steps), eta_min=3e-5)\n",
    "\n",
    "# Combine them\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "\n",
    "if checkpoint_filepath != None and checkpoint_filepath != \"\":\n",
    "    current_epoch = load_checkpoint(llm, optimizer, checkpoint_filepath) + 1\n",
    "else:\n",
    "    current_epoch = 0\n",
    "\n",
    "print(\"This model has\", sum(p.numel() for p in vit.parameters()), \"parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abf7e524-e5f1-4b9b-8955-9fb7488dfb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212849a-fad8-4fc9-bd48-5348a45f93c9",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26fdf79c-c835-4fc1-82dd-76927727f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 60013/60013 [25:14:37<00:00,  1.51s/it]\n",
      "100%|███████████████████████████████████████| 3159/3159 [21:57<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 0 as checkpoint_0_1.7800777179412202.pth.tar\n",
      "Training loss:  1.9168120420519592\n",
      "Validation loss:  1.7800777179412202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(current_epoch, epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "\n",
    "    vit.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        # Teacher forcing\n",
    "        input_data = data[:, :-1].long().to(device)\n",
    "        target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        prediction, load_balancing_loss = vit(input_data, [None] * input_data.shape[0])\n",
    "\n",
    "        # Change shape for loss calculation\n",
    "        prediction = prediction.view(-1, prediction.shape[-1])\n",
    "        target_data = target_data.reshape(-1)\n",
    "\n",
    "        loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    vit.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(validation_loader):\n",
    "            # Teacher forcing\n",
    "            input_data = data[:, :-1].long().to(device)\n",
    "            target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            prediction, load_balancing_loss = vit(input_data, [None] * input_data.shape[0])\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, prediction.shape[-1])\n",
    "            target_data = target_data.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "\n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "\n",
    "        loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(vit, optimizer, epoch, loss_valid[-1])\n",
    "\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    print(\"Training loss: \", loss_train[-1])\n",
    "    print(\"Validation loss: \", loss_valid[-1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7073d-1eec-47ae-819f-2ea4d52df671",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcd889-f1c8-46d7-8638-1e709083489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b692d-0473-409f-83df-8e9b69084a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46013d-1085-4d93-95b1-8ed02d8d9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"arXiv is an open-access\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "if tokenized_sentence[-1] == 2:\n",
    "    tokenized_sentence = tokenized_sentence[:-1]\n",
    "vit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_context_length): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence).unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction, _ = vit(tokenized_sentence_tensor, [None])\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "        \n",
    "tokens = tokenizer.decode(tokenized_sentence, skip_special_tokens=True)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
