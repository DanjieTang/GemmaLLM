{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0c2f9-f2cd-49cd-923c-965c1223c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54b2a9-0660-485f-9fe8-a57acf459683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.cuda import amp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ba7c2-10b8-45e7-aa4d-9d803da39851",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 1\n",
    "epochs = 30\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-3\n",
    "lr_min = 1e-5\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7759a0-79bd-460f-b236-5b2717bd0af2",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c21b9-0dfa-42ef-bf39-6ae2249ca910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pt file (If you had already preprocessed)\n",
    "word_embeddings_tensor = torch.load('word_embeddings_tensor_llama2.pt').cuda(device_id)\n",
    "vocabulary_size, embedding_dim = word_embeddings_tensor.shape\n",
    "zero_tensor = torch.zeros((1, 4096)).cuda(device_id)\n",
    "word_embeddings_tensor = torch.cat((word_embeddings_tensor, zero_tensor), dim=0)\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token_id = 32000\n",
    "pad_token_id = 32000\n",
    "max_token = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9407b-ca76-4923-9b0d-7be49a54d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row):\n",
    "    return tokenizer(row[\"dialog\"], max_length=max_token, truncation=False, padding=\"max_length\")\n",
    "\n",
    "def is_shorter_than_max_token(row):\n",
    "    \"\"\"\n",
    "    Return if a given row has more than max_token number of tokens\n",
    "    \"\"\"\n",
    "    return len(row['input_ids']) <= max_token\n",
    "\n",
    "def split_conversation(conversation): \n",
    "    \"\"\"\n",
    "    Split conversation into turns\n",
    "    \"\"\"\n",
    "    return [conversation[:i+2] for i in range(0, len(conversation), 2) if i+2 <= len(conversation)]\n",
    "\n",
    "def format_conversation(conversation: list[str]) -> str:\n",
    "    formatted_conversation = \"\"\n",
    "    \n",
    "    # Check if the conversation has more than two turns\n",
    "    if len(conversation) > 2:\n",
    "        # Process all but the last two turns\n",
    "        for i in range(len(conversation) - 2):\n",
    "            if i % 2 == 0:\n",
    "                formatted_conversation += \"<Past User>\" + conversation[i] + \"\\n\"\n",
    "            else:\n",
    "                formatted_conversation += \"<Past Assistant>\" + conversation[i] + \"\\n\"\n",
    "    \n",
    "    # Process the last two turns\n",
    "    if len(conversation) >= 2:\n",
    "        formatted_conversation += \"<User>\" + conversation[-2] + \"\\n\"\n",
    "        formatted_conversation += \"<Assistant>\" + conversation[-1]\n",
    "    \n",
    "    return formatted_conversation\n",
    "\n",
    "def convert_to_conversation(row):\n",
    "    conversation_list = row[\"dialog\"]\n",
    "    \n",
    "    conversation = format_conversation(conversation_list)\n",
    "    conversation += \"</s>\"\n",
    "    return {\"dialog\": conversation.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bde6c-d14b-4f5b-aea1-43a4fa82d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and tokenize dataset\n",
    "dataset = load_dataset(\"daily_dialog\", trust_remote_code=True)\n",
    "\n",
    "# 2. Split each conversation into multiple turns\n",
    "split_dataset = dataset.map(lambda x: {'dialog': split_conversation(x['dialog'])})\n",
    "\n",
    "# 3. Flatten each split so each row corresponds to a single turn\n",
    "flatten_dataset_train = [item for row in split_dataset[\"train\"][\"dialog\"] for item in row]\n",
    "flatten_dataset_valid = [item for row in split_dataset[\"validation\"][\"dialog\"] for item in row]\n",
    "flatten_dataset_test = [item for row in split_dataset[\"test\"][\"dialog\"] for item in row]\n",
    "\n",
    "# 4. Merge test data into the training data\n",
    "flatten_dataset_train.extend(flatten_dataset_test)  # merges test data into the training data\n",
    "\n",
    "# 5. Convert them back to Dataset objects\n",
    "flatten_dataset_train = Dataset.from_dict({'dialog': flatten_dataset_train})\n",
    "flatten_dataset_valid = Dataset.from_dict({'dialog': flatten_dataset_valid})\n",
    "# We skip flatten_dataset_test because we've merged it.\n",
    "\n",
    "# 6. Create a new DatasetDict without a test split\n",
    "dataset = DatasetDict({\n",
    "    'train': flatten_dataset_train,\n",
    "    'validation': flatten_dataset_valid\n",
    "})\n",
    "\n",
    "# 7. Convert entries to a conversational manner\n",
    "dataset = dataset.map(convert_to_conversation)\n",
    "\n",
    "# 8. Tokenize the dataset\n",
    "dataset = dataset.map(tokenize_function)\n",
    "\n",
    "# 9. (Optional) Filter out sequences exceeding your token limit\n",
    "dataset = dataset.filter(is_shorter_than_max_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f85e2-6831-4ebe-bede-19559d45f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sorted(dataset[\"train\"][\"input_ids\"], key=lambda lst: sum(1 for x in lst if x != 32000))\n",
    "validation_dataset = sorted(dataset[\"validation\"][\"input_ids\"], key=lambda lst: sum(1 for x in lst if x != 32000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f8fba-708a-471c-89e0-2694a113a6b6",
   "metadata": {},
   "source": [
    "# Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fba5a-61b5-413f-915b-e791e8be6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROPEEmbedding(nn.Module):\n",
    "    def __init__(self, max_context_length: int, head_dim: int = 64, theta: int = 10000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.create_embedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "\n",
    "    def create_embedding(self, max_context_length: int, head_dim: int = 64, theta: int = 10000) -> torch.Tensor:\n",
    "        tensor = torch.arange(0, head_dim // 2)\n",
    "        tensor = torch.repeat_interleave(tensor, 2)\n",
    "        tensor = -tensor * 2 / head_dim\n",
    "        tensor = torch.pow(theta, tensor)\n",
    "\n",
    "        index = torch.arange(max_context_length).float() # This is the m in the formula\n",
    "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
    "\n",
    "        cos_matrix = tensor.cos()\n",
    "        sin_matrix = tensor.sin()\n",
    "        sin_matrix[0::2] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
    "\n",
    "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
    "        pos_emb = pos_emb.transpose(1, 0)\n",
    "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def flip_for_sin(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        original_shape = tensor.shape\n",
    "        tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
    "        tensor = tensor[..., [1, 0]] # Swap\n",
    "        tensor = tensor.reshape(original_shape) # Get back to original shape\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
    "\n",
    "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
    "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
    "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
    "        tensor = cos + sin\n",
    "        return tensor\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.embedding = embedding\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Apply ROPE\n",
    "        query = self.embedding(query)\n",
    "        key = self.embedding(key)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(output)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        tensor = self.gate_and_up(tensor)\n",
    "        gate, up = tensor.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        tensor = self.down(tensor)\n",
    "        return tensor\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).cuda(device_id)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i]) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).cuda(device_id) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i]) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).cuda(device_id) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float16).cuda(device_id)\n",
    "        flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float16).cuda(device_id)\n",
    "        flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "class LLMLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.mqa = MultiQueryAttention(hidden_dim, head_dim, q_head, kv_head, embedding, lora_rank=lora_rank)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, num_experts=num_experts, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.mqa(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=tensor.device)# If not using MoE, load-balancing loss is zero\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 vocabulary_size: int,\n",
    "                 max_context_length: int,\n",
    "                 hidden_dim: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 projection_dim: int = None,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.embedding = ROPEEmbedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        # Because of computational power limit, we might want to project input token embedding down.\n",
    "        self.projection = projection_dim != None\n",
    "        if self.projection:\n",
    "            self.projection_matrix = nn.Linear(hidden_dim, projection_dim)\n",
    "            hidden_dim = projection_dim\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(LLMLayer(hidden_dim, head_dim, q_head, kv_head, self.embedding, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, use_moe=use_moe, num_experts=num_experts, lora_rank=lora_rank))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, vocabulary_size)\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"pos_emb\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # If projecting input embeddings\n",
    "        if self.projection:\n",
    "            tensor = self.projection_matrix(tensor)\n",
    "\n",
    "        seq_len = tensor.shape[1]\n",
    "        device_id = tensor.device\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).cuda(device_id)\n",
    "        causal_mask.requires_grad = False\n",
    "\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=device_id)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            tensor, load_balancing_loss = layer(tensor, attention_mask=causal_mask, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Classification\n",
    "        tensor = self.output_norm(tensor)\n",
    "        tensor = self.classifier(tensor)\n",
    "\n",
    "        return tensor, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391f7d0-88cf-47be-9861-e0a6786b4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = torch.load('200M Foundation Model.pth').cuda(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8994c0-35ad-4eb2-8593-50ead1f9c906",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61806ff1-e94e-4fb7-a0ee-856417bbaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.begin_fine_tunning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e03b8a-9c34-4361-bacb-8c73a3e24b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(llm.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*((len(dataset[\"train\"]) - 1) // batch_size + 1), eta_min=lr_min)\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18528f7f-abe1-43c8-a122-05bfc73bcf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "# For example, in a batch, the longest sentence length(including padding token) is 34. Then the training data become shape [batch_size, 34]\n",
    "def trim_padding(input_tensor):\n",
    "    # Create a mask where tokens are not equal to the pad_token\n",
    "    mask = input_tensor != pad_token_id  # Shape: [batch_size, max_seq_length]\n",
    "\n",
    "    # Calculate the lengths of each sentence (number of non-padding tokens)\n",
    "    lengths = mask.sum(dim=1)  # Shape: [batch_size]\n",
    "\n",
    "    # Find the maximum and minimum sentence lengths\n",
    "    max_length = lengths.max().item()\n",
    "    min_length = lengths.min().item()\n",
    "\n",
    "    # Trim the input tensor to the maximum sentence length\n",
    "    trimmed_tensor = input_tensor[:, :max_length]\n",
    "\n",
    "    return trimmed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03d054-0a0b-4db2-af98-fb4fd2d1e9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "    \n",
    "    llm.train()\n",
    "    for i in tqdm(range(0, len(train_dataset), batch_size)):\n",
    "        # Access data\n",
    "        end = min(i+batch_size, len(train_dataset))\n",
    "        data = torch.tensor(train_dataset[i:end])\n",
    "        data = trim_padding(data)\n",
    "\n",
    "        # Teacher forcing\n",
    "        input_data = data[:, :-1].long().cuda(device_id)\n",
    "        target_data = data[:, 1:].long().cuda(device_id)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data]\n",
    "\n",
    "        # Forward pass\n",
    "        with amp.autocast():\n",
    "            prediction, load_balancing_loss = llm(input_embeddings)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, vocabulary_size)\n",
    "            target_data = target_data.reshape(-1)\n",
    "\n",
    "            mask = target_data != pad_token_id\n",
    "            prediction = prediction[mask]\n",
    "            target_data = target_data[mask]\n",
    "\n",
    "            loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(llm.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Clear out grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    llm.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(validation_dataset), batch_size)):\n",
    "            # Access data\n",
    "            end = min(i+batch_size, len(validation_dataset))\n",
    "            data = torch.tensor(validation_dataset[i:end])\n",
    "            data = trim_padding(data)\n",
    "\n",
    "            # Teacher forcing\n",
    "            input_data = data[:, :-1].long().cuda(device_id)\n",
    "            target_data = data[:, 1:].long().cuda(device_id)\n",
    "\n",
    "            # Convert to embedding.\n",
    "            input_embeddings = word_embeddings_tensor[input_data]\n",
    "\n",
    "            # Forward pass\n",
    "            with amp.autocast():\n",
    "                prediction, load_balancing_loss = llm(input_embeddings)\n",
    "\n",
    "                # Change shape for loss calculation\n",
    "                prediction = prediction.view(-1, vocabulary_size)\n",
    "                target_data = target_data.reshape(-1)\n",
    "\n",
    "                mask = target_data != pad_token_id\n",
    "                prediction = prediction[mask]\n",
    "                target_data = target_data[mask]\n",
    "\n",
    "                loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "\n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "\n",
    "        loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    print(\"Training loss: \", loss_train[-1])\n",
    "    print(\"Validation loss: \", loss_valid[-1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb2dfa-ca23-443d-8b88-4782c750898d",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05afa5-75f6-4502-97e9-2c134488b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ce250-6a03-461f-947b-eba536e02b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def talk_with_llm(chat: str) -> str:\n",
    "    # Encode and move tensor into cuda if applicable.\n",
    "    conversation_history.append(chat)\n",
    "    conversation_history.append(\"\")\n",
    "    conversation = format_conversation(conversation_history)\n",
    "\n",
    "    tokenized_sentence = tokenizer(conversation)[\"input_ids\"]\n",
    "    if tokenized_sentence[-1] == 2:\n",
    "        tokenized_sentence = tokenized_sentence[:-1]\n",
    "    llm.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_token): # Keep iterating until reaches end of sentence or max token limit\n",
    "            # Preparing input\n",
    "            tokenized_sentence_tensor = torch.tensor(tokenized_sentence).cuda(device_id)\n",
    "            sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor]\n",
    "            sentence_embedding = sentence_embedding.unsqueeze(0).cuda(device_id)\n",
    "    \n",
    "            # Make prediction\n",
    "            with amp.autocast():\n",
    "                prediction, _ = llm(sentence_embedding)\n",
    "            prediction = prediction[0][-1] # We only care about last token\n",
    "            prediction = prediction / temperature\n",
    "            prediction = F.softmax(prediction, dim=-1)\n",
    "            output_token = torch.multinomial(prediction, 1)\n",
    "    \n",
    "            # Append to conversation history\n",
    "            tokenized_sentence.append(output_token.item())\n",
    "\n",
    "    response = tokenizer.decode(tokenized_sentence, skip_special_tokens=True)\n",
    "    response = response[len(conversation):]\n",
    "    \n",
    "    conversation_history.pop()\n",
    "    conversation_history.append(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd74d2-65ea-4770-bee6-2abedc138e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_with_llm(\"What's up homie, I'm Danjie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e703d-8ad3-47d3-bc11-fe6f22388640",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbff79-2c71-4198-80c4-7371640a51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(llm, 'chillGPT_200M.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
