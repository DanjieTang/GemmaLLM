# Dataset & Paths
train_path: languages_tokenized_50_train.npy
val_path: languages_tokenized_50_eval.npy
embeddings_path: word_embeddings_tensor_llama3.pt

# Model Architecture
num_layer: 3
max_context_length: 51
projection_dim: 512
expansion_factor: 16
q_head: 8
kv_head: 4

# Training Hyperparameters
epochs: 3
batch_size: 
 - 256
 - 128
 - 64
lr: 1e-3
weight_decay: 1e-3

# WandB
project: "VLM"
entity: "danjie-tang"