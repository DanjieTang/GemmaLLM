{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c5437-1af5-49f2-a923-b52633dbaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install nltk\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f98c81-8722-4497-8776-242a78e5ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danjietang/Documents/Github/GemmaLLM/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danjietang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/danjietang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import ijson\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\", user_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a8edc-2e8c-449f-a7a0-433eff784d1f",
   "metadata": {},
   "source": [
    "# Load dataset and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3f3e4-3f01-4bb0-badf-a19ba5614484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/clean-wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d68c2b-b8b5-4214-aebd-1e7a4c856a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire wikipedia\n",
    "with open(\"entire_wikipedia.jsonl\", 'w') as f:\n",
    "    for text_item in dataset[\"train\"][\"text\"]:\n",
    "        f.write(json.dumps(text_item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1716e3cb-03e8-43f0-996d-738c6377a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English wikipedia\n",
    "with open(\"english_wikipedia.jsonl\", 'w') as f:\n",
    "    for item in dataset[\"train\"]: # There's only train dataset loll\n",
    "        if item[\"wikicode\"] == \"en\":\n",
    "            f.write(json.dumps(item[\"text\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6748903-d884-47df-87de-0f152954d535",
   "metadata": {},
   "source": [
    "# Split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5ceaf-7831-4cfd-b6d1-f25522dfe18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "with open(\"english_wikipedia.jsonl\", 'r') as fin, open(\"english_wikipedia_sentences.jsonl\", 'w') as fout:\n",
    "    for line in tqdm(fin):\n",
    "        sentences = split_into_sentences(line)\n",
    "        for sentence in sentences:\n",
    "            fout.write(json.dumps(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f025ffc-85bc-480a-8ef7-8b8fa7d13d46",
   "metadata": {},
   "source": [
    "# Tokenize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7193e-e9bc-4001-9ed2-ccd1ecb25ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english_wikipedia.jsonl\", 'r') as fin, open(\"english_wikipedia_tokenized.jsonl\", 'w') as fout:\n",
    "    for line in tqdm(fin):\n",
    "        fout.write(json.dumps(tokenizer(line)[\"input_ids\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd78b1f-6912-44e2-8aab-21c5b13d5df3",
   "metadata": {},
   "source": [
    "# Count how many qualified sentences there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c017b2-0d0f-4f52-aaae-17fdb5535851",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 49\n",
    "min_token = 10\n",
    "counter = 0\n",
    "\n",
    "with open(\"english_wikipedia_tokenized.jsonl\", 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        tokenized_sentence = json.loads(line)\n",
    "        length = len(tokenized_sentence)\n",
    "        if length < max_token and length > min_token:\n",
    "            counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fc80b-fcc4-4856-b250-bd68b40c9252",
   "metadata": {},
   "source": [
    "# Store tokenized data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be681ff1-e0c4-4c1a-87ef-fa62b8399888",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 49\n",
    "min_token = 10\n",
    "shape = (counter, 50)\n",
    "counter = 0\n",
    "\n",
    "tokenized_tensor = torch.empty(shape, dtype=torch.int16)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "with open(\"english_wikipedia_tokenized.jsonl\", 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        tokenized_sentence = json.loads(line)\n",
    "        length = len(tokenized_sentence)\n",
    "        if length < max_token and length > min_token:\n",
    "            tokenized_sentence.append(eos_token_id)\n",
    "            tokenized_sentence = tokenized_sentence + [32000] * (50 - len(tokenized_sentence))\n",
    "            sentence_tokenized_tensor = torch.tensor(tokenized_sentence, dtype=torch.int16)\n",
    "            tokenized_tensor[counter] = sentence_tokenized_tensor\n",
    "            counter += 1\n",
    "\n",
    "torch.save(tokenized_tensor, \"llama2_wiki_50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d6be5-d723-4436-8dea-618700cc781d",
   "metadata": {},
   "source": [
    "# Split into train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518fdc0e-cf33-4660-ab3e-dd8da4ca88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No trim_padding\n",
    "tensor = torch.load(\"llama2_wiki_50.pt\")\n",
    "\n",
    "eval_ratio = 0.05\n",
    "n = tensor.shape[0]\n",
    "n_eval = int(n * eval_ratio)\n",
    "\n",
    "perm = torch.randperm(n)\n",
    "eval_indices = perm[:n_eval]\n",
    "train_indices = perm[n_e val:]\n",
    "\n",
    "# Sort indices to preserve original order\n",
    "eval_indices, _ = torch.sort(eval_indices)\n",
    "train_indices, _ = torch.sort(train_indices)\n",
    "\n",
    "# Split tensors while keeping order\n",
    "eval_tensor = tensor[eval_indices]\n",
    "train_tensor = tensor[train_indices]\n",
    "\n",
    "np.save(\"llama2_wiki_50_train.npy\", train_tensor.numpy())\n",
    "np.save(\"llama2_wiki_50_eval.npy\", eval_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8285e7-dac8-40c1-aea7-966ddfee5335",
   "metadata": {},
   "source": [
    "# Load token embedding and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f6f60-1612-4230-b0b8-564a973b26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Access the embedding matrix\n",
    "word_embeddings_tensor = model.model.embed_tokens.weight.data\n",
    "\n",
    "# Store vocabulary size and embedding dimension\n",
    "num_embeddings, embedding_dim = word_embeddings_tensor.shape\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "torch.save(word_embeddings_tensor, 'word_embeddings_tensor_llama3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba0aabe-055c-45b6-86ee-44746f41ee1f",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa003c-31e2-40d4-81ef-0b61026d5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.load(\"llama2_wiki_50.pt\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PAD_ID = 32000\n",
    "\n",
    "# Count how many padding tokens per row\n",
    "pad_counts = (tokens == PAD_ID).sum(dim=1)\n",
    "\n",
    "# Convert to CPU numpy for plotting\n",
    "pad_counts_np = pad_counts.cpu().numpy()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(pad_counts_np, bins=range(65), edgecolor='black', align='left')\n",
    "plt.title(\"Histogram of Padding Tokens per Sequence\")\n",
    "plt.xlabel(\"Number of Padding Tokens\")\n",
    "plt.ylabel(\"Number of Sequences\")\n",
    "plt.xlim(0, 64)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be026c5c-a358-4b0f-abed-d3539fe21865",
   "metadata": {},
   "source": [
    "# Preprocess multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f22269-17b8-4a89-9cab-cdb9601abc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# 2. Define a worker class to manage segmenters\n",
    "# We initialize segmenters once per process to save memory/time\n",
    "class SentenceProcessor:\n",
    "    def __init__(self):\n",
    "        self.segs = {\n",
    "            \"en\": pysbd.Segmenter(language=\"en\", clean=False),\n",
    "            \"es\": pysbd.Segmenter(language=\"es\", clean=False),\n",
    "            \"fr\": pysbd.Segmenter(language=\"fr\", clean=False),\n",
    "            \"de\": pysbd.Segmenter(language=\"de\", clean=False),\n",
    "            \"zh\": pysbd.Segmenter(language=\"zh\", clean=False),\n",
    "        }\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        counts = []\n",
    "        for code, text in zip(batch[\"wikicode\"], batch[\"text\"]):\n",
    "            if code not in self.segs:\n",
    "                counts.append(0)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # pysbd has a known regex bug in the German ('de') module\n",
    "                # wrapping this in a try-except prevents a full crash at 90%+\n",
    "                sentences = self.segs[code].segment(text)\n",
    "            except Exception:\n",
    "                # If segmentation fails, skip this specific document and continue\n",
    "                counts.append(0)\n",
    "                continue\n",
    "            \n",
    "            if not sentences:\n",
    "                counts.append(0)\n",
    "                continue\n",
    "\n",
    "            # Tokenize the whole list of sentences at once\n",
    "            tokenized = tokenizer(sentences, add_special_tokens=False)[\"input_ids\"]\n",
    "            \n",
    "            # Filter based on your criteria (len > 10 and len < 50)\n",
    "            valid_count = sum(1 for ids in tokenized if 9 < len(ids) < 49)\n",
    "            counts.append(valid_count)\n",
    "            \n",
    "        return {\"valid_sentence_count\": counts}\n",
    "\n",
    "# 3. Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"HuggingFaceFW/clean-wikipedia\", split=\"train\")\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = SentenceProcessor()\n",
    "\n",
    "    # Apply the mapping in parallel\n",
    "    # num_proc should generally be the number of CPU cores you have\n",
    "    processed_ds = dataset.map(\n",
    "        processor.process_batch,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        num_proc=os.cpu_count(),\n",
    "        remove_columns=dataset.column_names, # Clears memory by dropping raw text\n",
    "        desc=\"Counting qualifying sentences\"\n",
    "    )\n",
    "\n",
    "    # The result is exact because we sum the pre-calculated counts\n",
    "    final_total = sum(processed_ds[\"valid_sentence_count\"])\n",
    "\n",
    "    print(f\"\\nFinal counter value: {final_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f58b5-5941-4abc-b622-c29c16c2a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pysbd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "dataset = load_dataset(\"HuggingFaceFW/clean-wikipedia\", split=\"train\")\n",
    "\n",
    "# Pre-initialize segmenters in a dictionary for fast lookup\n",
    "segmenters = {\n",
    "    \"en\": pysbd.Segmenter(language=\"en\", clean=False),\n",
    "    \"es\": pysbd.Segmenter(language=\"es\", clean=False),\n",
    "    \"fr\": pysbd.Segmenter(language=\"fr\", clean=False),\n",
    "    \"de\": pysbd.Segmenter(language=\"de\", clean=False),\n",
    "    \"zh\": pysbd.Segmenter(language=\"zh\", clean=False)\n",
    "}\n",
    "\n",
    "def process_batch(batch):\n",
    "    all_filtered_ids = []\n",
    "    \n",
    "    for text, lang in zip(batch[\"text\"], batch[\"wikicode\"]):\n",
    "        if lang not in segmenters:\n",
    "            continue\n",
    "            \n",
    "        # 1. Segment sentences\n",
    "        try:\n",
    "            # pysbd has a known regex bug in the German ('de') module\n",
    "            # wrapping this in a try-except prevents a full crash at 90%+\n",
    "            sentences = segmenters[lang].segment(text)\n",
    "        except Exception:\n",
    "            # If segmentation fails, skip this specific document and continue\n",
    "            continue\n",
    "        \n",
    "        # 2. Tokenize sentences in a sub-batch (faster than one-by-one)\n",
    "        tokenized = tokenizer(sentences, add_special_tokens=False)[\"input_ids\"]\n",
    "        \n",
    "        # 3. Filter and add EOS token (128001)\n",
    "        for ids in tokenized:\n",
    "            if 9 < len(ids) < 49:\n",
    "                ids.append(128001)\n",
    "                all_filtered_ids.append(ids)\n",
    "                \n",
    "    return {\"token_ids\": all_filtered_ids}\n",
    "\n",
    "# Use .map with multiple processes\n",
    "# Adjust num_proc based on your Mac's core count (e.g., 8 or 10)\n",
    "processed_ds = dataset.map(\n",
    "    process_batch,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=os.cpu_count(), \n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Segmenting and Tokenizing\"\n",
    ")\n",
    "\n",
    "# Convert to final numpy array efficiently\n",
    "print(\"Creating final numpy array...\")\n",
    "rows = len(processed_ds)\n",
    "cols = 49\n",
    "# Using 128002 as padding value\n",
    "final_array = np.full((rows, cols), 128002, dtype=np.int32)\n",
    "\n",
    "for i, entry in enumerate(tqdm(processed_ds[\"token_ids\"], desc=f\"Filling Array\")):\n",
    "    length = len(entry)\n",
    "    final_array[i, :length] = entry\n",
    "\n",
    "np.save(\"languages_tokenized.npy\", final_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e2697-07b5-405c-a7f9-3e39dc50d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy array\n",
    "# Use mmap_mode=\"r\" if the file is very large\n",
    "data = np.load(\"languages_tokenized.npy\")\n",
    "\n",
    "eval_ratio = 0.05\n",
    "n = data.shape[0]\n",
    "n_eval = int(n * eval_ratio)\n",
    "\n",
    "# Generate shuffled indices\n",
    "perm = np.random.permutation(n)\n",
    "eval_indices = perm[:n_eval]\n",
    "train_indices = perm[n_eval:]\n",
    "\n",
    "# Split the array using the sorted indices\n",
    "eval_data = data[eval_indices]\n",
    "train_data = data[train_indices]\n",
    "\n",
    "# Save the results\n",
    "np.save(\"languages_tokenized_50_train.npy\", train_data[:20000000])\n",
    "np.save(\"languages_tokenized_50_eval.npy\", eval_data[:2000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef8340-fec3-4b65-a2f3-e386e26bfc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Access the embedding matrix\n",
    "word_embeddings_tensor = model.model.embed_tokens.weight.data\n",
    "\n",
    "# Store vocabulary size and embedding dimension\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "torch.save(word_embeddings_tensor, 'word_embeddings_tensor_llama3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610451c-ab6a-42a6-9c01-17f9963e8b94",
   "metadata": {},
   "source": [
    "# Given numpy array, add in image text training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b6648a-323e-4e86-b823-1d0f51e3207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"languages_tokenized.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3a81fc-01e3-40c2-8eea-6bd9cfc0eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(292951138, 49)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45b879-4a57-4385-b1c0-8d16ae3b69bc",
   "metadata": {},
   "source": [
    "# Make the numpy file smaller by filtering out unused indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8885c-ca6e-4d75-8b57-5db5b1a7e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "# 1. SCAN: Find which tokens are actually present\n",
    "@njit(parallel=True)\n",
    "def get_used_tokens_mask(data, max_token_id=128002):\n",
    "    # Create a small bitmask (128KB)\n",
    "    flags = np.zeros(max_token_id + 1, dtype=np.uint8)\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    \n",
    "    for i in prange(rows):\n",
    "        for j in range(cols):\n",
    "            token = data[i, j]\n",
    "            if token <= max_token_id:\n",
    "                flags[token] = 1\n",
    "    return flags\n",
    "\n",
    "# 2. MAP: Create the translation dictionary (Old ID -> New ID)\n",
    "def create_lookup_table(used_flags):\n",
    "    lookup = np.zeros_like(used_flags, dtype=np.int32)\n",
    "    current_new_idx = 0\n",
    "    \n",
    "    for i in range(len(used_flags)):\n",
    "        if used_flags[i] == 1:\n",
    "            lookup[i] = current_new_idx\n",
    "            current_new_idx += 1\n",
    "        else:\n",
    "            lookup[i] = -1 # Placeholder for unused tokens\n",
    "            \n",
    "    return lookup, current_new_idx\n",
    "\n",
    "# 3. APPLY: Overwrite the 54GB array in-place\n",
    "@njit(parallel=True)\n",
    "def apply_reindexing_inplace(data, lookup):\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    \n",
    "    for i in prange(rows):\n",
    "        for j in range(cols):\n",
    "            old_val = data[i, j]\n",
    "            # Directly overwrite the memory address\n",
    "            data[i, j] = lookup[old_val]\n",
    "\n",
    "print(\"Step 1: Scanning for used tokens...\")\n",
    "languages_tokenized_array = np.load(\"languages_tokenized.npy\")\n",
    "used_mask = get_used_tokens_mask(languages_tokenized_array)\n",
    "\n",
    "print(\"Step 2: Creating re-indexing map...\")\n",
    "mapping_table, total_unique = create_lookup_table(used_mask)\n",
    "print(f\"Original Range: 0-128002 | New Range: 0-{total_unique - 1}\")\n",
    "np.save(\"mapping_table.npy\", mapping_table)\n",
    "\n",
    "print(\"Step 3: Applying transformation in-place (no extra RAM used)...\")\n",
    "apply_reindexing_inplace(languages_tokenized_array, mapping_table)\n",
    "\n",
    "print(\"Success! Your array has been compacted.\")\n",
    "np.save(\"languages_tokenized_sliced.npy\", languages_tokenized_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bd6c1-7f34-43e1-8817-4c08492ba910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy array\n",
    "# Use mmap_mode=\"r\" if the file is very large\n",
    "data = np.load(\"languages_tokenized_sliced.npy\")\n",
    "\n",
    "eval_ratio = 0.05\n",
    "n = data.shape[0]\n",
    "n_eval = int(n * eval_ratio)\n",
    "\n",
    "# Generate shuffled indices\n",
    "perm = np.random.permutation(n)\n",
    "eval_indices = perm[:n_eval]\n",
    "train_indices = perm[n_eval:]\n",
    "\n",
    "# Sort indices to preserve original order\n",
    "eval_indices.sort()\n",
    "train_indices.sort()\n",
    "\n",
    "# Split the array using the sorted indices\n",
    "eval_data = data[eval_indices]\n",
    "train_data = data[train_indices]\n",
    "\n",
    "# Save the results\n",
    "np.save(\"languages_tokenized_50_train_sliced.npy\", train_data)\n",
    "np.save(\"languages_tokenized_50_eval_sliced.npy\", eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9fbe2-b57b-4bf2-a8c1-a7c9f3b92ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Get the indices of the tokens you actually kept\n",
    "# 'used_mask' is the boolean array from our previous script\n",
    "used_indices = np.where(used_mask)[0]\n",
    "\n",
    "# 2. Convert to a torch tensor\n",
    "indices_to_keep = torch.tensor(used_indices, dtype=torch.long)\n",
    "\n",
    "# 3. Slice the embedding matrix\n",
    "# This selects only the rows we need, in the order of our new IDs\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "word_embeddings_tensor = model.model.embed_tokens.weight.data\n",
    "pruned_embeddings = word_embeddings_tensor[indices_to_keep]\n",
    "\n",
    "print(f\"Original shape: {word_embeddings_tensor.shape}\")\n",
    "print(f\"Pruned shape:   {pruned_embeddings.shape}\")\n",
    "\n",
    "# 4. Save the pruned version\n",
    "torch.save(pruned_embeddings, 'word_embeddings_llama3_sliced.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
