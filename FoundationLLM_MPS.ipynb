{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GnaYhv053_B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import math\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xo9Azch255he"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_context_length = 64\n",
    "device = \"cpu\"\n",
    "epochs = 10\n",
    "batch_size = 512 # Must be the same as preprocessing batch_size\n",
    "validation_batch_size = 10\n",
    "weight_decay = 1e-3\n",
    "lr = 1e-3\n",
    "num_layer = 3\n",
    "head_dim = 64\n",
    "projection_dim = 1024\n",
    "expansion_factor = 8\n",
    "checkpoint_filepath = \"\"\n",
    "training_data_path = \"llama2_wiki_64_ranked_train.npy\"\n",
    "eval_data_path = \"llama2_wiki_64_ranked_eval.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_KJtX6t58ho"
   },
   "source": [
    "# Load llama2 token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVoaUSPI59BD"
   },
   "outputs": [],
   "source": [
    "# Load from pt file (If you had already preprocessed)\n",
    "word_embeddings_tensor = torch.load('word_embeddings_tensor_llama2.pt').to(device)\n",
    "vocabulary_size, embedding_dim = word_embeddings_tensor.shape\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "pad_token_id = 32000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2IIm9lI5_rb"
   },
   "source": [
    "# Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # Create memory-mapped array\n",
    "        self.mmap_data = np.load(filename, mmap_mode='r')\n",
    "        self.length = self.mmap_data.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.mmap_data[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset(training_data_path)\n",
    "eval_dataset = Dataset(eval_data_path)\n",
    "\n",
    "training_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(eval_dataset, batch_size=validation_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pnpcy7m6IaR"
   },
   "source": [
    "# Instantiate LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO3pGAwp6HOy"
   },
   "outputs": [],
   "source": [
    "class ROPEEmbedding(nn.Module):\n",
    "    def __init__(self, max_context_length: int, head_dim: int = 64, theta: int = 10000):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.create_embedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "\n",
    "    def create_embedding(self, max_context_length: int, head_dim: int = 64, theta: int = 10000) -> torch.Tensor:\n",
    "        tensor = torch.arange(0, head_dim // 2)\n",
    "        tensor = torch.repeat_interleave(tensor, 2)\n",
    "        tensor = -tensor * 2 / head_dim\n",
    "        tensor = torch.pow(theta, tensor)\n",
    "\n",
    "        index = torch.arange(max_context_length).float() # This is the m in the formula\n",
    "        tensor = torch.einsum(\"i, j -> ij\", tensor, index)\n",
    "\n",
    "        cos_matrix = tensor.cos()\n",
    "        sin_matrix = tensor.sin()\n",
    "        sin_matrix[0::2] *= -1 # Flipping sign for 0, 2, 4... row of sin matrix\n",
    "\n",
    "        pos_emb = torch.cat((cos_matrix, sin_matrix), dim=0)\n",
    "        pos_emb = pos_emb.transpose(1, 0)\n",
    "        pos_emb = nn.Parameter(pos_emb, requires_grad=False)\n",
    "\n",
    "        return pos_emb\n",
    "\n",
    "    def flip_for_sin(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        original_shape = tensor.shape\n",
    "        tensor = tensor.reshape(tensor.shape[0], tensor.shape[1], -1, 2) # Get to pairs\n",
    "        tensor = tensor[..., [1, 0]] # Swap\n",
    "        tensor = tensor.reshape(original_shape) # Get back to original shape\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        sequence_length = tensor.shape[2] # Assuming we are using batch_size, head, sequence_length and dim\n",
    "\n",
    "        tensor = torch.cat((tensor, self.flip_for_sin(tensor)), dim=-1)\n",
    "        tensor = tensor * self.pos_emb[:sequence_length, :]\n",
    "        cos, sin = tensor.chunk(chunks=2, dim=-1)\n",
    "        tensor = cos + sin\n",
    "        return tensor\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.embedding = embedding\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Apply ROPE\n",
    "        query = self.embedding(query)\n",
    "        key = self.embedding(key)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(value)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.lora_gate_and_up_a = nn.Linear(hidden_size, lora_rank)\n",
    "        self.lora_gate_and_up_b = nn.Linear(lora_rank, hidden_size * expansion_factor * 2)\n",
    "        self.lora_down_a = nn.Linear(hidden_size * expansion_factor, lora_rank)\n",
    "        self.lora_down_b = nn.Linear(lora_rank, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        gate_and_up = self.gate_and_up(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_gate_and_up_a(tensor)\n",
    "            lora_tensor = self.lora_gate_and_up_b(lora_tensor)\n",
    "            gate_and_up = gate_and_up + lora_tensor\n",
    "        gate, up = gate_and_up.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        down_tensor = self.down(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_down_a(tensor)\n",
    "            lora_tensor = self.lora_down_b(lora_tensor)\n",
    "            down_tensor = down_tensor + lora_tensor\n",
    "        return down_tensor\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1, lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).to(device)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i], fine_tuning) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(device) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i], fine_tuning) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(device) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(device)\n",
    "        flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(device)\n",
    "        flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "class LLMLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 embedding: ROPEEmbedding,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.mqa = MultiQueryAttention(hidden_dim, head_dim, q_head, kv_head, embedding, lora_rank=lora_rank)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, num_experts=num_experts, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.mqa(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor, fine_tuning=fine_tuning)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor, fine_tuning=fine_tuning)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=tensor.device)# If not using MoE, load-balancing loss is zero\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layer: int,\n",
    "                 vocabulary_size: int,\n",
    "                 max_context_length: int,\n",
    "                 hidden_dim: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = None,\n",
    "                 kv_head: int = None,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 theta: int = 10000,\n",
    "                 projection_dim: int = None,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts=8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.embedding = ROPEEmbedding(max_context_length, head_dim=head_dim, theta=theta)\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        # Because of computational power limit, we might want to project input token embedding down.\n",
    "        self.projection = projection_dim != None\n",
    "        if self.projection:\n",
    "            self.projection_matrix = nn.Linear(hidden_dim, projection_dim)\n",
    "            hidden_dim = projection_dim\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(LLMLayer(hidden_dim, head_dim, q_head, kv_head, self.embedding, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, use_moe=use_moe, num_experts=num_experts, lora_rank=lora_rank))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, vocabulary_size)\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"pos_emb\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        # If projecting input embeddings\n",
    "        if self.projection:\n",
    "            tensor = self.projection_matrix(tensor)\n",
    "\n",
    "        seq_len = tensor.shape[1]\n",
    "        device_id = tensor.device\n",
    "\n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(device)\n",
    "        causal_mask.requires_grad = False\n",
    "\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=device_id)\n",
    "\n",
    "        for layer in self.transformer:\n",
    "            tensor, load_balancing_loss = layer(tensor, attention_mask=causal_mask, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Classification\n",
    "        tensor = self.output_norm(tensor)\n",
    "        tensor = self.classifier(tensor)\n",
    "\n",
    "        return tensor, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEoFvTp86Lrc"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model_to_save = model.module\n",
    "    else:\n",
    "        model_to_save = model\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_to_save.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'checkpoint_{epoch}_{loss}.pth.tar'\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f'Checkpoint saved at epoch {epoch} as {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6WbVfDz6NO6"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, filename) -> int:\n",
    "    checkpoint = torch.load(filename)\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Checkpoint loaded from epoch {epoch} with loss {loss}')\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cghnRdmA6P5u"
   },
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(num_layer, vocabulary_size, max_context_length, embedding_dim, projection_dim=projection_dim, expansion_factor=expansion_factor, use_moe=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtM0fmTu6Owz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(llm.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(training_loader), eta_min=1e-5)\n",
    "\n",
    "if checkpoint_filepath != None and checkpoint_filepath != \"\":\n",
    "    current_epoch = load_checkpoint(llm, optimizer, checkpoint_filepath) + 1\n",
    "else:\n",
    "    current_epoch = 0\n",
    "\n",
    "print(\"This model has\", sum(p.numel() for p in llm.parameters()), \"parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raKMhFsH6R7a"
   },
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "loss_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnZ3T8Zr6TKU"
   },
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "# For example, in a batch, the longest sentence length(including padding token) is 34. Then the training data become shape [batch_size, 34]\n",
    "def trim_padding(input_tensor):\n",
    "    # Create a mask where tokens are not equal to the pad_token\n",
    "    mask = input_tensor != pad_token_id  # Shape: [batch_size, max_seq_length]\n",
    "\n",
    "    # Calculate the lengths of each sentence (number of non-padding tokens)\n",
    "    lengths = mask.sum(dim=1)  # Shape: [batch_size]\n",
    "\n",
    "    # Find the maximum and minimum sentence lengths\n",
    "    max_length = lengths.max().item()\n",
    "    min_length = lengths.min().item()\n",
    "\n",
    "    # Check if the difference between the longest and shortest sentence is 2 or more\n",
    "    if max_length - min_length >= 2:\n",
    "        return None\n",
    "\n",
    "    # Trim the input tensor to the maximum sentence length\n",
    "    trimmed_tensor = input_tensor[:, :max_length]\n",
    "\n",
    "    return trimmed_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGhdIzou6VuJ"
   },
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVuUWMZi6WGw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(current_epoch, epochs):\n",
    "    loss_train_epoch = []\n",
    "    loss_val_epoch = []\n",
    "\n",
    "    llm.train()\n",
    "    for data in tqdm(training_loader):\n",
    "        # Teacher forcing\n",
    "        data = trim_padding(data)\n",
    "        if data == None:\n",
    "            continue\n",
    "        \n",
    "        input_data = data[:, :-1].long().to(device)\n",
    "        target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "        # Convert to embedding.\n",
    "        input_embeddings = word_embeddings_tensor[input_data].float()\n",
    "\n",
    "        # Forward pass\n",
    "        prediction, load_balancing_loss = llm(input_embeddings)\n",
    "\n",
    "        # Change shape for loss calculation\n",
    "        prediction = prediction.view(-1, vocabulary_size)\n",
    "        target_data = target_data.reshape(-1)\n",
    "\n",
    "        mask = target_data != pad_token_id\n",
    "        prediction = prediction[mask]\n",
    "        target_data = target_data[mask]\n",
    "\n",
    "        loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(llm.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Record loss\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_train.append(np.mean(loss_train_epoch))\n",
    "\n",
    "    llm.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(validation_loader):\n",
    "            # Teacher forcing\n",
    "            data = trim_padding(data)\n",
    "            if data == None:\n",
    "                continue\n",
    "            input_data = data[:, :-1].long().to(device)\n",
    "            target_data = data[:, 1:].long().to(device)\n",
    "\n",
    "            # Convert to embedding.\n",
    "            input_embeddings = word_embeddings_tensor[input_data].float()\n",
    "\n",
    "            # Forward pass\n",
    "            prediction, load_balancing_loss = llm(input_embeddings)\n",
    "\n",
    "            # Change shape for loss calculation\n",
    "            prediction = prediction.view(-1, vocabulary_size)\n",
    "            target_data = target_data.reshape(-1)\n",
    "\n",
    "            mask = target_data != pad_token_id\n",
    "            prediction = prediction[mask]\n",
    "            target_data = target_data[mask]\n",
    "\n",
    "            loss = criterion(prediction, target_data) + load_balancing_loss # Calculate loss\n",
    "\n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "\n",
    "        loss_valid.append(np.mean(loss_val_epoch))\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(llm, optimizer, epoch, loss_valid[-1])\n",
    "\n",
    "    plt.plot(loss_train, label=\"Training loss\")\n",
    "    plt.plot(loss_valid, label=\"Validation loss\")\n",
    "    print(\"Training loss: \", loss_train[-1])\n",
    "    print(\"Validation loss: \", loss_valid[-1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJVIzTMY6ZJN"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNyTMWf96YXZ"
   },
   "outputs": [],
   "source": [
    "temperature = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = torch.load('200M Foundation Model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV_d0pfS6b8j"
   },
   "outputs": [],
   "source": [
    "sentence = \"arXiv is an open-access\"\n",
    "tokenized_sentence = tokenizer(sentence)[\"input_ids\"]\n",
    "if tokenized_sentence[-1] == 2:\n",
    "    tokenized_sentence = tokenized_sentence[:-1]\n",
    "llm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(tokenized_sentence[-1] != tokenizer.eos_token_id and len(tokenized_sentence) < max_context_length): # Keep iterating until reaches end of sentence or max token limit\n",
    "        # Preparing input\n",
    "        tokenized_sentence_tensor = torch.tensor(tokenized_sentence).to(device)\n",
    "        sentence_embedding = word_embeddings_tensor[tokenized_sentence_tensor].float()\n",
    "        sentence_embedding = sentence_embedding.unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        prediction, _ = llm(sentence_embedding)\n",
    "        prediction = prediction[0][-1] # We only care about last token\n",
    "        prediction = prediction / temperature\n",
    "        prediction = F.softmax(prediction, dim=-1)\n",
    "        output_token = torch.multinomial(prediction, 1)\n",
    "\n",
    "        # Append to conversation history\n",
    "        tokenized_sentence.append(output_token.item())\n",
    "        \n",
    "tokens = tokenizer.decode(tokenized_sentence, skip_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObMjR6Ib6dnW"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IeAiQg-6e7u"
   },
   "outputs": [],
   "source": [
    "torch.save(llm, 'llm3point156.pth')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
