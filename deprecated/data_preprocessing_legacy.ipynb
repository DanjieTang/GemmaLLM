{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3de21-8909-4200-8da8-340f79252f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install tqdm\n",
    "!pip install nltk\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4e38-bf80-4d1a-9fc5-bc598d172363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import ijson\n",
    "import requests\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\", user_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcab10f",
   "metadata": {},
   "source": [
    "# Load dataset and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578caa5-57e1-4ef7-a995-9555663ae68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/clean-wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1933388-9483-4e02-95b2-3f8a95067512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire wikipedia\n",
    "with open(\"entire_wikipedia.jsonl\", 'w') as f:\n",
    "    for text_item in dataset[\"train\"][\"text\"]:\n",
    "        f.write(json.dumps(text_item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2325d-c0e6-4415-95ce-2ffec399cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English, Spanish, French, German, Mandarin wikipedia\n",
    "with open(\"languages_wikipedia.jsonl\", 'w') as f:\n",
    "    for item in tqdm(dataset[\"train\"]): # There's only train dataset loll\n",
    "        if item[\"wikicode\"] == \"en\" or item[\"wikicode\"] == \"es\" or item[\"wikicode\"] == \"fr\" or item[\"wikicode\"] == \"de\" or item[\"wikicode\"] == \"zh\":\n",
    "            f.write(json.dumps(item[\"text\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a60d77-f454-4883-a82c-d78e7a96c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English wikipedia\n",
    "with open(\"english_wikipedia.jsonl\", 'w') as f:\n",
    "    for item in dataset[\"train\"]: # There's only train dataset loll\n",
    "        if item[\"wikicode\"] == \"en\":\n",
    "            f.write(json.dumps(item[\"text\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd58e6",
   "metadata": {},
   "source": [
    "# Split into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b290c1a-c436-4445-9599-611a3e8484e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "with open(\"english_wikipedia.jsonl\", 'r') as fin, open(\"english_wikipedia_sentences.jsonl\", 'w') as fout:\n",
    "    for line in tqdm(fin):\n",
    "        sentences = split_into_sentences(line)\n",
    "        for sentence in sentences:\n",
    "            fout.write(json.dumps(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5035fe-9461-4db4-bc57-37bcae4e9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "segmenter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "\n",
    "# Split into sentences for multiple languages\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    return segmenter.segment(text)\n",
    "\n",
    "with open(\"english_wikipedia.jsonl\", 'r') as fin, open(\"english_wikipedia_sentences.jsonl\", 'w') as fout:\n",
    "    for line in tqdm(fin):\n",
    "        sentences = split_into_sentences(line)\n",
    "        for sentence in sentences:\n",
    "            fout.write(json.dumps(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813691f",
   "metadata": {},
   "source": [
    "# Tokenize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f297e-5445-4b3b-8345-c93b862b9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"english_wikipedia.jsonl\", 'r') as fin, open(\"english_wikipedia_tokenized.jsonl\", 'w') as fout:\n",
    "    for line in tqdm(fin):\n",
    "        fout.write(json.dumps(tokenizer(line)[\"input_ids\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d96a59",
   "metadata": {},
   "source": [
    "# Count how many qualified sentences there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1bb0e-54de-44cc-bedb-94dbe9daff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 49\n",
    "min_token = 10\n",
    "counter = 0\n",
    "\n",
    "with open(\"english_wikipedia_tokenized.jsonl\", 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        tokenized_sentence = json.loads(line)\n",
    "        length = len(tokenized_sentence)\n",
    "        if length < max_token and length > min_token:\n",
    "            counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75304ca0-0535-40cb-9f18-e5a784c73971",
   "metadata": {},
   "source": [
    "# Store tokenized data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9975bf-17ac-47be-aaaa-0d5163e44c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = 49\n",
    "min_token = 10\n",
    "shape = (counter, 50)\n",
    "counter = 0\n",
    "\n",
    "tokenized_tensor = torch.empty(shape, dtype=torch.int16)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "with open(\"english_wikipedia_tokenized.jsonl\", 'r') as file:\n",
    "    for line in tqdm(file):\n",
    "        tokenized_sentence = json.loads(line)\n",
    "        length = len(tokenized_sentence)\n",
    "        if length < max_token and length > min_token:\n",
    "            tokenized_sentence.append(eos_token_id)\n",
    "            tokenized_sentence = tokenized_sentence + [32000] * (50 - len(tokenized_sentence))\n",
    "            sentence_tokenized_tensor = torch.tensor(tokenized_sentence, dtype=torch.int16)\n",
    "            tokenized_tensor[counter] = sentence_tokenized_tensor\n",
    "            counter += 1\n",
    "\n",
    "torch.save(tokenized_tensor, \"llama2_wiki_50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b093c5-7211-481e-ab43-dab011fba710",
   "metadata": {},
   "source": [
    "# Sort the tensor based on number of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3417a5e8-3557-4a68-a911-8f4153d2a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.load(\"llama2_wiki_50.pt\")\n",
    "pad_token_id = 32000\n",
    "\n",
    "lengths = (tensor != pad_token_id).sum(dim=1)\n",
    "sorted_lengths, sorted_indices = torch.sort(lengths)\n",
    "sorted_tensor = tensor[sorted_indices]\n",
    "\n",
    "torch.save(sorted_tensor, \"llama2_wiki_50_ranked.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6f34c-442b-49ed-9d43-ca43f5fcb278",
   "metadata": {},
   "source": [
    "# Split into train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4e5d6-b1f9-4cbf-b3fc-8e4867824cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For trim_padding\n",
    "tensor = torch.load(\"llama2_wiki_50_ranked.pt\")\n",
    "\n",
    "eval_ratio = 0.05\n",
    "n = tensor.shape[0]\n",
    "n_eval = int(n * eval_ratio)\n",
    "\n",
    "perm = torch.randperm(n)\n",
    "eval_indices = perm[:n_eval]\n",
    "train_indices = perm[n_eval:]\n",
    "\n",
    "# Sort indices to preserve original order\n",
    "eval_indices, _ = torch.sort(eval_indices)\n",
    "train_indices, _ = torch.sort(train_indices)\n",
    "`\n",
    "# Split tensors while keeping order\n",
    "eval_tensor = tensor[eval_indices]\n",
    "train_tensor = tensor[train_indices]\n",
    "\n",
    "torch.save(eval_tensor, \"llama2_wiki_50_ranked_eval_sorted.pt\")\n",
    "torch.save(train_tensor, \"llama2_wiki_50_ranked_train_sorted.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fae29a-d6e6-4acf-ba92-d3d7ca6aa0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No trim_padding\n",
    "tensor = torch.load(\"llama2_wiki_50.pt\")\n",
    "\n",
    "eval_ratio = 0.05\n",
    "n = tensor.shape[0]\n",
    "n_eval = int(n * eval_ratio)\n",
    "\n",
    "perm = torch.randperm(n)\n",
    "eval_indices = perm[:n_eval]\n",
    "train_indices = perm[n_eval:]\n",
    "\n",
    "# Sort indices to preserve original order\n",
    "eval_indices, _ = torch.sort(eval_indices)\n",
    "train_indices, _ = torch.sort(train_indices)\n",
    "\n",
    "# Split tensors while keeping order\n",
    "eval_tensor = tensor[eval_indices]\n",
    "train_tensor = tensor[train_indices]\n",
    "\n",
    "np.save(\"llama2_wiki_50_train.npy\", train_tensor.numpy())\n",
    "np.save(\"llama2_wiki_50_eval.npy\", eval_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ad786-6b77-4dac-92e9-cfd317ee11b6",
   "metadata": {},
   "source": [
    "# Batch the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983360c-088d-438c-ac1a-0a54a37c2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_chunks(tensor, chunk_size, generator: torch.Generator):\n",
    "    n_full = tensor.shape[0] // chunk_size\n",
    "    perm_chunks = torch.randperm(n_full, generator=generator)\n",
    "\n",
    "    idx_full = torch.arange(n_full * chunk_size).view(n_full, chunk_size)\n",
    "    idx_shuffled = idx_full[perm_chunks].reshape(-1)\n",
    "\n",
    "    return tensor[idx_shuffled]\n",
    "\n",
    "train_tensor = torch.load(\"llama2_wiki_50_ranked_train_sorted.pt\")\n",
    "eval_tensor = torch.load(\"llama2_wiki_50_ranked_eval_sorted.pt\")\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "batch_size = 256\n",
    "\n",
    "train_tensor_shuffled = shuffle_in_chunks(train_tensor, chunk_size=batch_size, generator=g).numpy()\n",
    "eval_tensor_shuffled = shuffle_in_chunks(eval_tensor, chunk_size=batch_size, generator=g).numpy()\n",
    "\n",
    "np.save(\"llama2_wiki_50_train_new_batch_256.npy\", train_tensor_shuffled)\n",
    "np.save(\"llama2_wiki_50_eval_new_batch_256.npy\", eval_tensor_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10640389",
   "metadata": {},
   "source": [
    "# Load token embedding and store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddb729-1139-4d18-9eea-7a9f93a23889",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\")\n",
    "tokenizer.pad_token_id = 128002\n",
    "\n",
    "# Access the embedding matrix\n",
    "word_embeddings_tensor = model.model.embed_tokens.weight.data\n",
    "\n",
    "# Delete llama3 because we are no longer using it.\n",
    "del model\n",
    "\n",
    "# Store vocabulary size and embedding dimension\n",
    "num_embeddings, embedding_dim = word_embeddings_tensor.shape\n",
    "word_embeddings_tensor.requires_grad = False\n",
    "\n",
    "torch.save(word_embeddings_tensor, 'word_embeddings_tensor_llama2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace02889-31f8-413a-b10a-5e538f6d2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.load(\"llama2_wiki_64_ranked.pt\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PAD_ID = 32000\n",
    "\n",
    "# Count how many padding tokens per row\n",
    "pad_counts = (tokens == PAD_ID).sum(dim=1)\n",
    "\n",
    "# Convert to CPU numpy for plotting\n",
    "pad_counts_np = pad_counts.cpu().numpy()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(pad_counts_np, bins=range(65), edgecolor='black', align='left')\n",
    "plt.title(\"Histogram of Padding Tokens per Sequence\")\n",
    "plt.xlabel(\"Number of Padding Tokens\")\n",
    "plt.ylabel(\"Number of Sequences\")\n",
    "plt.xlim(0, 64)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
